{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c761882c",
   "metadata": {},
   "source": [
    "# Requirments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a16573bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Install required versions\n",
    "# !pip install transformers==4.49.0 \\\n",
    "#             scikit-learn==1.5.2 \\\n",
    "#             numpy==2.1.2 \\\n",
    "#             openai==1.54.4 \\\n",
    "#             google-cloud-aiplatform==1.72.0 \\\n",
    "#             pandas==2.2.3 \\\n",
    "#             Pillow==11.0.0 \\\n",
    "#             opencv-python==4.10.0 \\\n",
    "#             requests==2.32.3 \\\n",
    "#             qwen-vl-utils==0.0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce00e978-510b-4de7-9550-62c06ac0713a",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddbc1a08-ee49-4aef-b1df-1569e2a8c135",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import PIL\n",
    "import pandas as pd\n",
    "import json\n",
    "import requests\n",
    "import transformers\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "\n",
    "PROJECT_PATH = './expirments' # Path to expirments folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0762886e-2eaa-4b9f-85d8-39c6249a5868",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178f1122-3fea-4991-a040-c0c390b19497",
   "metadata": {},
   "source": [
    "### General"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548ea9c0-a7f1-4c6b-8fba-db5658705b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import base64\n",
    "import json\n",
    "from PIL import Image\n",
    "import nltk\n",
    "import zlib\n",
    "import math\n",
    "import hashlib\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def display_title(title, font_size=24):\n",
    "    display(HTML(f\"<h1 style='font-size: {font_size}px;'>{title}</h1>\"))\n",
    "\n",
    "def convert(o):\n",
    "    if isinstance(o, np.int64) or isinstance(o, np.int32):\n",
    "        return int(o)\n",
    "    raise TypeError\n",
    "\n",
    "def dump_json(file_path, data):\n",
    "    with open(file_path, 'w') as f:\n",
    "        string_json = json.dumps(data, cls=NumpyEncoder)\n",
    "        f.write(string_json)\n",
    "        f.close()\n",
    "\n",
    "def create_json(path):\n",
    "    if not os.path.isfile(path):\n",
    "        print('Creating json ', path)\n",
    "        with io.open(path, 'w+') as json_file:\n",
    "            json_file.write(json.dumps({}))\n",
    "    return path\n",
    "\n",
    "def flatten(lst):\n",
    "    flat_list = []\n",
    "    for item in lst:\n",
    "        if isinstance(item, list) or isinstance(item, set):\n",
    "            flat_list.extend(flatten(item))\n",
    "        else:\n",
    "            flat_list.append(item)\n",
    "    return flat_list\n",
    "\n",
    "def pil_to_html(image, size=100):\n",
    "    buffered = io.BytesIO()\n",
    "    image.save(buffered, format=\"PNG\")\n",
    "    img_str = base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
    "    return f'<img src=\"data:image/png;base64,{img_str}\" alt=\"Product Image\" style=\"max-width:{size}px;\">'\n",
    "\n",
    "def display_images_in_dataframe(data_df, image_colums=['source_img', 'target_img', 'mask_img', 'source_mask_bbox_padding', 'target_mask_bbox_padding'], index_start=0, index_end=5, size=100, drop_columns=None):\n",
    "    data_df = data_df[index_start:index_end]\n",
    "    data_df = pd.DataFrame(data_df)\n",
    "\n",
    "    if drop_columns:\n",
    "        data_df.drop(columns=drop_columns, inplace=True, errors='ignore')\n",
    "\n",
    "    data_frame_colums = list(data_df.columns)\n",
    "    for column in image_colums:\n",
    "        if column in data_frame_colums:\n",
    "            data_df[column] = data_df[column].map(lambda x: pil_to_html(x, size))\n",
    "\n",
    "    # Convert the DataFrame to HTML\n",
    "    df_html = data_df.to_html(escape=False)\n",
    "    \n",
    "    # Wrap the DataFrame HTML in a div that has a horizontal scrollbar\n",
    "    html = f'''\n",
    "    <div style=\"overflow-x: auto; border: 1px solid #ccc; margin-bottom: 10px;\">\n",
    "        {df_html}\n",
    "    </div>\n",
    "    '''\n",
    "    display(HTML(html))\n",
    "    \n",
    "def get_json(file_path, DataFrame=False):\n",
    "    with open(os.path.join(file_path),  encoding=\"utf8\") as f:\n",
    "        response = json.load(f)\n",
    "        f.close()\n",
    "        if DataFrame:\n",
    "            return pd.DataFrame(response)\n",
    "        return response\n",
    "    \n",
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    \"\"\" Custom encoder for numpy data types and specific handling for 'pred_masks'.\"\"\"\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return {'type': 'ndarray', 'data': obj.tolist(), 'shape': obj.shape}  # Include shape information\n",
    "        elif isinstance(obj, (np.float32, np.float64, np.int32, np.int64)):\n",
    "            return float(obj)  # Convert NumPy floats to Python float\n",
    "        return json.JSONEncoder.default(self, obj)\n",
    "\n",
    "def decode_element(element):\n",
    "    \"\"\"Recursively decode JSON element, converting any special encoded numpy arrays back.\"\"\"\n",
    "    if isinstance(element, dict):\n",
    "        if 'type' in element and element['type'] == 'ndarray':\n",
    "            # This is an encoded numpy array, convert it back\n",
    "            return np.array(element['data']).reshape(element['shape'])\n",
    "        else:\n",
    "            # Otherwise, recursively decode dictionary elements\n",
    "            return {key: decode_element(value) for key, value in element.items()}\n",
    "    elif isinstance(element, list):\n",
    "        # Recursively decode list elements\n",
    "        return [decode_element(item) for item in element]\n",
    "    else:\n",
    "        # Return the element unchanged if it's not a list or dictionary\n",
    "        return element\n",
    "\n",
    "def compress_segmentation(segmentation_array):\n",
    "    array_int = segmentation_array.astype(int)\n",
    "    bit_string = ''.join(map(str, array_int.flatten()))\n",
    "    byte_array = bytearray(int(bit_string[i : i + 8], 2) for i in range(0, len(bit_string), 8))\n",
    "    # Compress the byte array\n",
    "    compressed_data = zlib.compress(byte_array)\n",
    "    compressed_data_base64 = base64.b64encode(compressed_data).decode('utf-8')\n",
    "    return compressed_data_base64\n",
    "\n",
    "def decompresse_segmentation(compressed_segmentation_array):\n",
    "    decompressed_data_bytes = base64.b64decode(compressed_segmentation_array)\n",
    "    # Decompress\n",
    "    decompressed_data = zlib.decompress(decompressed_data_bytes)\n",
    "    # Convert back to an array (you need to know the original shape and content type)\n",
    "    original_bit_string = ''.join(format(byte, '08b') for byte in decompressed_data)\n",
    "    width = height = int(math.sqrt(len(original_bit_string)))\n",
    "    original_array = np.array([int(bit) for bit in original_bit_string]).reshape((width, height))\n",
    "    return original_array\n",
    "\n",
    "def get_image_id(image: Image.Image) -> str:\n",
    "    \"\"\"\n",
    "    Generate a unique string ID for a given PIL image using an MD5 hash of the pixel data.\n",
    "    \"\"\"\n",
    "    # Convert the image to raw bytes\n",
    "    img_bytes = image.tobytes()\n",
    "\n",
    "    # Create an MD5 hash object and update it with the image bytes\n",
    "    hash_md5 = hashlib.md5()\n",
    "    hash_md5.update(img_bytes)\n",
    "\n",
    "    # Return the hexadecimal digest of the hash as the unique ID\n",
    "    return hash_md5.hexdigest()\n",
    "\n",
    "def is_pil_image(image):\n",
    "    return isinstance(image, Image.Image)\n",
    "\n",
    "def get_instance_id(example, original=False):\n",
    "    return f\"{example['img_id']}_{example['original_instruction'] if original else example['instruction']}_{example['turn_index']}\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08b89f1",
   "metadata": {},
   "source": [
    "###  Load Cache Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04805b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "GEMINI_CACHE_PATH = create_json(PROJECT_PATH + '/merged_gemini_cache.json')\n",
    "GPT_CACHE_PATH = create_json(PROJECT_PATH + '/merged_gpt_cache.json')\n",
    "QWEN_CACHE_PATH = create_json(PROJECT_PATH + '/qwen_cache.json')\n",
    "INTERN_VL3_CACHE_PATH  = create_json(PROJECT_PATH + '/intern_vl3_cache.json')\n",
    "GEMINI_CACHE = get_json(GEMINI_CACHE_PATH)\n",
    "GPT_CACHE = get_json(GPT_CACHE_PATH)\n",
    "\n",
    "LOCAL_RUN = False # use cache only no apis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1a9754-0511-41d6-a514-7aed5eb7d9e7",
   "metadata": {},
   "source": [
    "### Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d8b4fb-be9f-404a-ba1f-22be41ce0c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set the environment variable to point to the credentials file\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = './application_default_credentials.json'\n",
    "\n",
    "from vertexai.preview.generative_models import (\n",
    "    GenerationConfig,\n",
    "    GenerativeModel,\n",
    "    Image as PartImage\n",
    ")\n",
    "\n",
    "GEMINI_MAX_TOKENS = 1000 \n",
    "GEMINI_TEMPRATURE = 0\n",
    "GEMINI_MODEL = \"gemini-pro-vision\"\n",
    "GEMINI_MODE_one_half = \"gemini-1.5-pro-preview-0409\"\n",
    "\n",
    "objects_list_generation_config = GenerationConfig(\n",
    "    temperature=GEMINI_TEMPRATURE,\n",
    "    top_p=1.0,\n",
    "    top_k=32,\n",
    "    candidate_count=1,\n",
    "    max_output_tokens=GEMINI_MAX_TOKENS,\n",
    ")\n",
    "\n",
    "# Define project information\n",
    "PROJECT_ID = \"gen-lang-client-0642013381\"  # @param {type:\"string\"}\n",
    "LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
    "\n",
    "# Initialize Vertex AI\n",
    "import vertexai\n",
    "vertexai.init(project=PROJECT_ID, location=LOCATION)\n",
    "multimodal_model = GenerativeModel(GEMINI_MODEL)\n",
    "multimodal_model_one_half = GenerativeModel(GEMINI_MODE_one_half)\n",
    "\n",
    "def get_gemini_cache_id(prompt):\n",
    "    return f\"{prompt}___{GEMINI_MODEL}___{GEMINI_MAX_TOKENS}___{GEMINI_TEMPRATURE}\"\n",
    "\n",
    "def get_gemini_response_from_cache(prompt):\n",
    "    id = get_gemini_cache_id(prompt).replace('\\r', '')\n",
    "    return GEMINI_CACHE.get(id)\n",
    "\n",
    "def save_gemini_response_in_cache(prompt, response):\n",
    "    GEMINI_CACHE.update({get_gemini_cache_id(prompt): response})\n",
    "    dump_json(GEMINI_CACHE_PATH, GEMINI_CACHE)\n",
    "\n",
    "def get_multimodal_model(mode_name):\n",
    "    if mode_name == GEMINI_MODEL:\n",
    "        return multimodal_model\n",
    "    elif mode_name == GEMINI_MODE_one_half:\n",
    "        return multimodal_model_one_half\n",
    "    raise Exception()\n",
    "\n",
    "# This is the image type required by Gemini API\n",
    "def get_part_image(image):\n",
    "    img_byte_arr = io.BytesIO()\n",
    "    image.save(img_byte_arr, format=image.format if image.format is not None else 'PNG')\n",
    "    return PartImage.from_bytes(img_byte_arr.getvalue())\n",
    "\n",
    "def get_original_unparsed_gemini_response(responses, contents, model, debug=False, generation_config=None):\n",
    "    model = get_multimodal_model(model)\n",
    "    if generation_config is None:\n",
    "        responses_generator = model.generate_content(contents, stream=True)\n",
    "    else:\n",
    "        responses_generator = model.generate_content(contents, stream=True, generation_config=generation_config)\n",
    "    for response in responses_generator:\n",
    "        try:\n",
    "            responses.append(response.text) # there is more metadata here, for now we ingnore this\n",
    "        except Exception as e:\n",
    "            debug and print('Error reading Gemini response, Error:', e)\n",
    "            debug and print('Contents', contents)\n",
    "    debug and print(responses)\n",
    "    return responses\n",
    "\n",
    "def get_geimini_response(contents, model=GEMINI_MODEL, debug=False, generation_config=objects_list_generation_config, overide_cache=False):\n",
    "    debug and print(contents)\n",
    "    \n",
    "    instance_id = '' if model == GEMINI_MODEL else f'{model}-'\n",
    "    for index, obj in enumerate(contents):\n",
    "        instance_id = instance_id + get_image_id(obj) if isinstance(obj, PIL.Image.Image) else instance_id +obj\n",
    "\n",
    "    cached_response = get_gemini_response_from_cache(instance_id)\n",
    "\n",
    "    if cached_response is not None and not overide_cache and (cached_response != [] and cached_response != ['']):\n",
    "        return ''.join(cached_response).strip()\n",
    "\n",
    "    for index, obj in enumerate(contents):\n",
    "        if isinstance(obj, PIL.Image.Image):\n",
    "            contents[index] = get_part_image(obj)\n",
    "\n",
    "    if LOCAL_RUN:\n",
    "        raise Exception()\n",
    "    \n",
    "    debug and print(contents)\n",
    "    try:\n",
    "        responses = get_original_unparsed_gemini_response([], contents, model, debug, generation_config=generation_config) # first try\n",
    "        save_gemini_response_in_cache(instance_id, responses)\n",
    "    except:\n",
    "        try:\n",
    "            responses = get_original_unparsed_gemini_response([], contents, model, debug, generation_config=generation_config) # second try\n",
    "            save_gemini_response_in_cache(instance_id, responses)\n",
    "        except Exception as e:\n",
    "            print('Gemini Error tryed two time!\\n', e)\n",
    "            responses = []\n",
    "   \n",
    "    return ''.join(responses).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b715c6-5e1e-44b9-9867-d287cf79d2e5",
   "metadata": {},
   "source": [
    "### NLTK utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0967a47a-ebbd-41df-bbce-ebe1f41d8663",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# Ensure necessary NLTK resources are downloaded\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "def evaluate_captions(reference_caption, generated_caption):\n",
    "    reference_tokens = nltk.word_tokenize(reference_caption)\n",
    "    generated_tokens = nltk.word_tokenize(generated_caption)\n",
    "    meteor = meteor_score([reference_tokens], generated_tokens)\n",
    "    rouge_scorer_inst = rouge_scorer.RougeScorer(['rouge1'], use_stemmer=True)\n",
    "    rouge1_recall = rouge_scorer_inst.score(reference_caption, generated_caption)['rouge1'].recall\n",
    "    reference_embedding = model.encode(reference_caption, convert_to_tensor=True)\n",
    "    generated_embedding = model.encode(generated_caption, convert_to_tensor=True)\n",
    "    similarity = util.pytorch_cos_sim(reference_embedding, generated_embedding).item()\n",
    "    return meteor, rouge1_recall, similarity\n",
    "\n",
    "def shortest_path_distance(word1, word2):\n",
    "    # Get synsets for both words\n",
    "    synsets1 = wn.synsets(word1)\n",
    "    synsets2 = wn.synsets(word2)\n",
    "    \n",
    "    # Initialize the shortest path distance to None\n",
    "    shortest_distance = None\n",
    "    \n",
    "    # Compare each synset of word1 against each synset of word2\n",
    "    for synset1 in synsets1:\n",
    "        for synset2 in synsets2:\n",
    "            # Compute the shortest path distance between synset1 and synset2\n",
    "            distance = synset1.shortest_path_distance(synset2)\n",
    "            if distance is not None:\n",
    "                # If it's the first distance found or if it's shorter than the previous shortest, update\n",
    "                if shortest_distance is None or distance < shortest_distance:\n",
    "                    shortest_distance = distance\n",
    "    \n",
    "    return shortest_distance\n",
    "\n",
    "def get_related_synonyms(word, level=3):\n",
    "    synonyms = set()\n",
    "    synset = wn.synsets(word, pos=wn.NOUN)\n",
    "    if len(synset) >= 1:\n",
    "        synset = synset[0]\n",
    "        # Direct synonyms\n",
    "        synonyms.update(lemma.name() for lemma in synset.lemmas())\n",
    "        \n",
    "        # Explore one level of hypernyms (more general terms)\n",
    "        if level >= 2:\n",
    "            for hypernym in synset.hypernyms():\n",
    "                synonyms.update(lemma.name() for lemma in hypernym.lemmas())\n",
    "            \n",
    "        # Explore one level of hyponyms (more specific terms)\n",
    "        if level >= 3:\n",
    "            for hyponym in synset.hyponyms():\n",
    "                synonyms.update(lemma.name() for lemma in hyponym.lemmas())\n",
    "            \n",
    "        return list(synonyms)\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "def is_noun(word):\n",
    "    # Tokenize the word\n",
    "    tokens = word_tokenize(word)\n",
    "    # POS tag the tokenized word\n",
    "    tagged = pos_tag(tokens)\n",
    "    # Check if the POS tag of the word is one of the noun tags\n",
    "    return tagged[0][1] in [\"NN\", \"NNS\", \"NNP\", \"NNPS\"]\n",
    "\n",
    "# Example usage\n",
    "word = \"backpack\"\n",
    "related_synonyms = get_related_synonyms(word)\n",
    "print(f\"Related synonyms for '{word}':\", related_synonyms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d2bc52-00d7-4baa-9575-645883443ac9",
   "metadata": {},
   "source": [
    "### GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27206717-b681-4bd6-952d-cdbe96b21784",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from io import BytesIO\n",
    "\n",
    "API_KEY = \"\"\n",
    "client = OpenAI(api_key=API_KEY)\n",
    "gpt_counter = 0\n",
    "# Params\n",
    "MAX_TOKENS = 500\n",
    "TEMPERATURE = 0.000001  # Default 1\n",
    "\n",
    "def get_gpt_cache_id(prompt, model, temperature = TEMPERATURE, max_tokens=MAX_TOKENS):\n",
    "    return f\"{prompt}___{model}___{temperature}_____{max_tokens}\".replace('\\r', '')\n",
    "\n",
    "def get_gpt_response_from_cache(prompt, model, temperature = TEMPERATURE, max_tokens=MAX_TOKENS):\n",
    "    id = get_gpt_cache_id(prompt, model, temperature, max_tokens)\n",
    "    return GPT_CACHE.get(id)\n",
    "\n",
    "def save_gpt_response_in_cache(prompt, model, response, temprature=TEMPERATURE, max_tokens=MAX_TOKENS):\n",
    "    global gpt_counter\n",
    "    GPT_CACHE.update({get_gpt_cache_id(prompt, model, temprature, max_tokens): response})\n",
    "    gpt_counter += 1\n",
    "    if gpt_counter % 100 == 0:\n",
    "        print(gpt_counter, 'saving cache')\n",
    "        dump_json(GPT_CACHE_PATH, GPT_CACHE)\n",
    "\n",
    "def encode_image(image):\n",
    "    buffered = BytesIO()\n",
    "    image.save(buffered, format=\"PNG\")\n",
    "    return base64.b64encode(buffered.getvalue()).decode('utf-8')\n",
    "    \n",
    "def get_gpt_4_vision_response(contents, MODEL=\"gpt-4-vision-preview\", debug=False):\n",
    "    instance_id = ''\n",
    "    debug and print(contents)\n",
    "\n",
    "    for index, obj in enumerate(contents):\n",
    "        instance_id = instance_id + get_image_id(obj) if isinstance(obj, PIL.Image.Image) else instance_id + obj\n",
    "\n",
    "    cached_response = get_gpt_response_from_cache(instance_id, MODEL)\n",
    "    \n",
    "    if cached_response is not None:\n",
    "        return cached_response\n",
    "    \n",
    "    if LOCAL_RUN:\n",
    "        raise Exception()\n",
    "    \n",
    "    for index, obj in enumerate(contents):\n",
    "        if isinstance(obj, PIL.Image.Image) or isinstance(obj, str):\n",
    "            contents[index] = {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{encode_image(obj)}\",  \"detail\": \"low\"}} if isinstance(obj, PIL.Image.Image) else {\"type\": \"text\", \"text\": contents[index]}\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(model=MODEL,messages=[{\"role\": \"user\",\"content\": contents}], temperature=TEMPERATURE,max_tokens=1000)\n",
    "    except:\n",
    "        try:\n",
    "            response = client.chat.completions.create(model=MODEL,messages=[{\"role\": \"user\",\"content\": contents}], temperature=TEMPERATURE,max_tokens=1000)\n",
    "        except Exception as e:\n",
    "            print('GPT4 Vision request failed twice' , e)\n",
    "            raise Exception('GPT4 Vision request failed twice' )\n",
    "\n",
    "    debug and print(response)\n",
    "    save_gpt_response_in_cache(instance_id, MODEL, response.choices[0].message.content)\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def get_chatgpt_4_prediction(prompt, overide_cache=False, temprature=TEMPERATURE, max_tokens=MAX_TOKENS):\n",
    "    model=\"gpt-4\"\n",
    "    try:\n",
    "        response = get_chat_gpt_prediction(prompt, model, overide_cache, temprature=temprature, max_tokens=max_tokens)\n",
    "    except:\n",
    "        try:\n",
    "            response = get_chat_gpt_prediction(prompt, model, overide_cache, temprature=temprature, max_tokens=max_tokens)\n",
    "        except Exception as e:\n",
    "            print('GPT request failed twice' , e)\n",
    "            raise Exception('GPT request failed twice')\n",
    "    return response\n",
    "\n",
    "def get_chat_gpt_prediction(prompt, model, overide_cache=False, temprature=TEMPERATURE, max_tokens=MAX_TOKENS):\n",
    "    cached_response = get_gpt_response_from_cache(prompt, model, temprature, max_tokens) if not overide_cache else None\n",
    "\n",
    "    if cached_response is not None:\n",
    "        return cached_response\n",
    "    if LOCAL_RUN:\n",
    "        raise Exception()\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "      model=model,\n",
    "      max_tokens=1000,\n",
    "      temperature=temprature,\n",
    "      messages=[\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "      ]\n",
    "    )\n",
    "    save_gpt_response_in_cache(prompt, model, response.choices[0].message.content, temprature, max_tokens)\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9fe01d-1ad6-4e6c-8cb9-39a246187cbc",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc6c3c3-11f1-46d6-aec1-c34329bbbfdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning, message=\".*ANTIALIAS is deprecated.*\")\n",
    "\n",
    "def get_annotated_data_with_majority():\n",
    "    df = pd.read_csv(r'./editinspector_benchmark.csv', index_col=0)\n",
    "    for col in df.columns:\n",
    "        if col.startswith('metadata_'):\n",
    "            df[col] = df[col].apply(eval)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd2e60d-dd75-4a03-8c80-d285e33871ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_MB_brush_test_set():\n",
    "    test_folder = 'PATH TO MAGICBRUSH TEST SET'\n",
    "    images_folder = test_folder + '/images'\n",
    "    test = get_json(test_folder + '/edit_sessions.json')\n",
    "    \n",
    "    # ['img_id', 'turn_index', 'source_img', 'mask_img', 'instruction', 'target_img']\n",
    "    def get_turn_index(example):\n",
    "        try:\n",
    "            return int(example['input'].split('.')[0][-1])+1\n",
    "        except:\n",
    "            return 1\n",
    "    \n",
    "    formated_edits = []\n",
    "    for key in test.keys():\n",
    "        edits = test.get(key)\n",
    "        for edit in edits:\n",
    "            formated_edit = dict()\n",
    "            formated_edit['img_id'] = key\n",
    "            formated_edit['instruction'] = edit['instruction']\n",
    "            formated_edit['source_img'] = Image.open(images_folder + fr'/{key}/{edit[\"input\"]}')\n",
    "            formated_edit['mask_img'] = Image.open(images_folder + fr'/{key}/{edit[\"mask\"]}')\n",
    "            formated_edit['target_img'] = Image.open(images_folder + fr'/{key}/{edit[\"output\"]}')\n",
    "            formated_edit['turn_index'] = get_turn_index(edit)\n",
    "            formated_edits.append(formated_edit)\n",
    "    return pd.DataFrame(formated_edits)\n",
    "\n",
    "\n",
    "MB_test = get_MB_brush_test_set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dae8986-98b3-4496-9d7a-c86a48a2b05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "def is_pil_image(image):\n",
    "    return isinstance(image, Image.Image)\n",
    "\n",
    "def is_nan_or_none(value):\n",
    "    return value is None or (isinstance(value, float) and math.isnan(value))\n",
    "\n",
    "def get_MB_example_idx(example, MB_set):\n",
    "    example_img_id = example['id'].split('_')[0]\n",
    "    turn_index = int(example['id'].split('_')[-1])\n",
    "    example_instruction = example.get('original_instruction') if not is_nan_or_none(example.get('original_instruction')) else example['instruction']\n",
    "    indices_instruction = set([i for i, x in enumerate(MB_set['instruction']) if x == example_instruction])\n",
    "    indices_img_id = set([i for i, x in enumerate(MB_set['img_id']) if x == example_img_id])\n",
    "    indices_turn = set([i for i, x in enumerate(MB_set['turn_index']) if x == turn_index])\n",
    "    example_index = indices_instruction.intersection(indices_img_id).intersection(indices_turn)\n",
    "    assert len(example_index) == 1\n",
    "    return example_index.pop()\n",
    "\n",
    "def smooth_dall_e_pixels(image):\n",
    "    \"\"\"\n",
    "    Smooths the DALLÂ·E watermark region by copying pixels from nearby areas and using inpainting.\n",
    "    The region is defined as the bottom-right corner (x: 92.1% to 100% of width, y: 98.3% to 100% of height).\n",
    "    \n",
    "    :param image: PIL Image object to which the smoothing will be applied.\n",
    "    :return: Modified image with inpainting applied.\n",
    "    \"\"\"\n",
    "    width, height = image.size\n",
    "\n",
    "    # Calculate the region coordinates for the image\n",
    "    x_start = int(width * 0.921)\n",
    "    x_end = width\n",
    "    y_start = int(height * 0.983)\n",
    "    y_end = height\n",
    "\n",
    "    # Convert the PIL image to a NumPy array\n",
    "    image_np = np.array(image)\n",
    "\n",
    "    # Create a mask for inpainting\n",
    "    mask = np.zeros(image_np.shape[:2], np.uint8)\n",
    "\n",
    "    # Mark the region in the mask\n",
    "    mask[y_start:y_end, x_start:x_end] = 255\n",
    "\n",
    "    # Apply inpainting to smooth the transition\n",
    "    inpainted_image = cv2.inpaint(image_np, mask, inpaintRadius=3, flags=cv2.INPAINT_TELEA)\n",
    "\n",
    "    # Convert the inpainted result back to a PIL image\n",
    "    inpainted_image_pil = Image.fromarray(inpainted_image)\n",
    "\n",
    "    return inpainted_image_pil\n",
    "\n",
    "images_cache = dict()\n",
    "def load_example_images(example, smooth_images=True):\n",
    "    global images_cache\n",
    "    for MB_set in [MB_test]:\n",
    "        try:\n",
    "            example_index = get_MB_example_idx(example, MB_set)\n",
    "            example_images = images_cache.get(example_index)\n",
    "            if example_images is None:\n",
    "                example_images = [MB_set.iloc[example_index]['source_img'], MB_set.iloc[example_index]['target_img']] \n",
    "                images_cache.update({example_index: example_images})\n",
    "            example['source_img'] = example_images[0].copy()\n",
    "            example['target_img'] = example_images[1].copy()\n",
    "            if smooth_images:\n",
    "                source_img_size = example['source_img'].size\n",
    "                example['target_img'] = smooth_dall_e_pixels(example['target_img'].resize(source_img_size) if example['target_img'].size != source_img_size else example['target_img'])\n",
    "                example['source_img'] = smooth_dall_e_pixels(example['source_img'])\n",
    "        except Exception:\n",
    "            continue\n",
    "            \n",
    "    assert is_pil_image(example['source_img']) and is_pil_image(example['target_img'])\n",
    "    return example \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbc01e6-b4fd-4e5f-965e-36f18fac95d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def smart_convert_yes_no_list(values):\n",
    "    \"\"\"\n",
    "    Convert variations of \"Yes\"/\"No\" strings (e.g., \"Yes\", \"No\", \" yes \", etc.) to True/False.\n",
    "    Handles cases where the response starts with \"Yes\" or \"No\" but may have additional text.\n",
    "    Raises an exception if a string is not recognized as \"Yes\" or \"No\".\n",
    "    Accepts a list or pandas Series.\n",
    "    \"\"\"\n",
    "    # Regular expression patterns for matching Yes/No variants\n",
    "    yes_pattern = re.compile(r'^\\s*(yes|no contradiction)\\b.*$', re.IGNORECASE)\n",
    "    no_pattern = re.compile(r'^\\s*(no|contradiction found)\\b.*$', re.IGNORECASE)\n",
    "\n",
    "    def map_to_bool(value):\n",
    "        \"\"\"Map Yes/No variations to True/False using regex matching or raise an error.\"\"\"\n",
    "        if isinstance(value, str):\n",
    "            if yes_pattern.match(value):\n",
    "                return True\n",
    "            elif no_pattern.match(value):\n",
    "                return False\n",
    "            else:\n",
    "                print(f'Received invalid answer: \"{value}\", treating this as \"No\".')\n",
    "                return False\n",
    "                # raise Exception(f\"Invalid value for Yes/No conversion: '{value}'\")\n",
    "        return value  # Return original non-string values without conversion\n",
    "\n",
    "    # Apply the mapping function to each value\n",
    "    if isinstance(values, pd.Series):\n",
    "        return values.apply(map_to_bool)\n",
    "    else:\n",
    "        return [map_to_bool(v) for v in values]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8898ff0",
   "metadata": {},
   "source": [
    "## Qwen2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57fa7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import hashlib\n",
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "qwen_model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-VL-7B-Instruct\", torch_dtype=torch.bfloat16, device_map=\"auto\"\n",
    ")\n",
    "qwen_processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-7B-Instruct\")\n",
    "qwen_model.eval()\n",
    "\n",
    "\n",
    "if os.path.exists(QWEN_CACHE_PATH):\n",
    "    try:\n",
    "        with open(QWEN_CACHE_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "            QWEN_CACHE = json.load(f)\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Warning: Cache file was corrupt or empty. Starting with empty cache.\")\n",
    "        QWEN_CACHE = {}\n",
    "else:\n",
    "    QWEN_CACHE = {}\n",
    "\n",
    "def manual_save_qwen_one():\n",
    "    with open(QWEN_CACHE_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(QWEN_CACHE, f, indent=2)\n",
    "\n",
    "qwen_counter = 0\n",
    "def save_qwen_cache():\n",
    "    global qwen_counter\n",
    "    qwen_counter+=1\n",
    "    if qwen_counter % 10 == 0:\n",
    "        with open(QWEN_CACHE_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(QWEN_CACHE, f, indent=2)\n",
    "\n",
    "def make_cache_key(before_image: Image.Image, after_image: Image.Image, prompt: str) -> str:\n",
    "    before_id = get_image_id(before_image)\n",
    "    after_id = get_image_id(after_image)\n",
    "    return f\"{before_id}_{after_id}_{hashlib.md5(prompt.encode('utf-8')).hexdigest()}\"\n",
    "\n",
    "# --- Main Function ---\n",
    "def get_qwen_response(inputs):\n",
    "    with torch.no_grad():\n",
    "        before_image, after_image, prompt = inputs\n",
    "        cache_key = make_cache_key(before_image, after_image, prompt)\n",
    "\n",
    "        if cache_key in QWEN_CACHE:\n",
    "            print(\"[CACHE HIT]\")\n",
    "            return QWEN_CACHE[cache_key]\n",
    "\n",
    "        print(\"[CACHE MISS] Running model...\")\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": [\n",
    "                {\"type\": \"image\", \"image\": before_image},\n",
    "                {\"type\": \"image\", \"image\": after_image},\n",
    "                {\"type\": \"text\", \"text\": prompt},\n",
    "            ]}\n",
    "        ]\n",
    "\n",
    "        # Preparation for inference\n",
    "        text = qwen_processor.apply_chat_template(\n",
    "            messages, tokenize=False, add_generation_prompt=True\n",
    "        )\n",
    "        image_inputs, video_inputs = process_vision_info(messages)\n",
    "        inputs = qwen_processor(\n",
    "            text=[text],\n",
    "            images=image_inputs,\n",
    "            videos=video_inputs,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        inputs = inputs.to(\"cuda\")\n",
    "\n",
    "        # Inference: Generation of the output\n",
    "        generated_ids = qwen_model.generate(**inputs, max_new_tokens=265)\n",
    "        generated_ids_trimmed = [\n",
    "            out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "        ]\n",
    "        response = qwen_processor.batch_decode(\n",
    "            generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "        )\n",
    "\n",
    "        final_response = response[0]\n",
    "        QWEN_CACHE[cache_key] = final_response\n",
    "        save_qwen_cache()\n",
    "        return final_response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02022ad1",
   "metadata": {},
   "source": [
    "## InternVL3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fb085a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from transformers import AutoModel, AutoConfig, AutoTokenizer\n",
    "generation_config = dict(max_new_tokens=1024, do_sample=True)\n",
    "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
    "IMAGENET_STD = (0.229, 0.224, 0.225)\n",
    "\n",
    "generation_config = dict(max_new_tokens=1024, do_sample=True)\n",
    "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
    "IMAGENET_STD = (0.229, 0.224, 0.225)\n",
    "\n",
    "def build_transform(input_size):\n",
    "    MEAN, STD = IMAGENET_MEAN, IMAGENET_STD\n",
    "    transform = T.Compose([\n",
    "        T.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),\n",
    "        T.Resize((input_size, input_size), interpolation=InterpolationMode.BICUBIC),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=MEAN, std=STD)\n",
    "    ])\n",
    "    return transform\n",
    "\n",
    "def find_closest_aspect_ratio(aspect_ratio, target_ratios, width, height, image_size):\n",
    "    best_ratio_diff = float('inf')\n",
    "    best_ratio = (1, 1)\n",
    "    area = width * height\n",
    "    for ratio in target_ratios:\n",
    "        target_aspect_ratio = ratio[0] / ratio[1]\n",
    "        ratio_diff = abs(aspect_ratio - target_aspect_ratio)\n",
    "        if ratio_diff < best_ratio_diff:\n",
    "            best_ratio_diff = ratio_diff\n",
    "            best_ratio = ratio\n",
    "        elif ratio_diff == best_ratio_diff:\n",
    "            if area > 0.5 * image_size * image_size * ratio[0] * ratio[1]:\n",
    "                best_ratio = ratio\n",
    "    return best_ratio\n",
    "\n",
    "def dynamic_preprocess(image, min_num=1, max_num=12, image_size=448, use_thumbnail=False):\n",
    "    orig_width, orig_height = image.size\n",
    "    aspect_ratio = orig_width / orig_height\n",
    "\n",
    "    # calculate the existing image aspect ratio\n",
    "    target_ratios = set(\n",
    "        (i, j) for n in range(min_num, max_num + 1) for i in range(1, n + 1) for j in range(1, n + 1) if\n",
    "        i * j <= max_num and i * j >= min_num)\n",
    "    target_ratios = sorted(target_ratios, key=lambda x: x[0] * x[1])\n",
    "\n",
    "    # find the closest aspect ratio to the target\n",
    "    target_aspect_ratio = find_closest_aspect_ratio(\n",
    "        aspect_ratio, target_ratios, orig_width, orig_height, image_size)\n",
    "\n",
    "    # calculate the target width and height\n",
    "    target_width = image_size * target_aspect_ratio[0]\n",
    "    target_height = image_size * target_aspect_ratio[1]\n",
    "    blocks = target_aspect_ratio[0] * target_aspect_ratio[1]\n",
    "\n",
    "    # resize the image\n",
    "    resized_img = image.resize((target_width, target_height))\n",
    "    processed_images = []\n",
    "    for i in range(blocks):\n",
    "        box = (\n",
    "            (i % (target_width // image_size)) * image_size,\n",
    "            (i // (target_width // image_size)) * image_size,\n",
    "            ((i % (target_width // image_size)) + 1) * image_size,\n",
    "            ((i // (target_width // image_size)) + 1) * image_size\n",
    "        )\n",
    "        # split the image\n",
    "        split_img = resized_img.crop(box)\n",
    "        processed_images.append(split_img)\n",
    "    assert len(processed_images) == blocks\n",
    "    if use_thumbnail and len(processed_images) != 1:\n",
    "        thumbnail_img = image.resize((image_size, image_size))\n",
    "        processed_images.append(thumbnail_img)\n",
    "    return processed_images\n",
    "\n",
    "\n",
    "\n",
    "def load_image(image, input_size=448, max_num=12):\n",
    "    # Load image from URL or local path\n",
    "    if isinstance(image, str):\n",
    "        if image.startswith('http://') or image.startswith('https://'):\n",
    "            response = requests.get(image)\n",
    "            response.raise_for_status()\n",
    "            image = Image.open(BytesIO(response.content)).convert('RGB')\n",
    "        else:\n",
    "            image = Image.open(image).convert('RGB')\n",
    "\n",
    "    # Apply preprocessing\n",
    "    transform = build_transform(input_size=input_size)\n",
    "    images = dynamic_preprocess(image, image_size=input_size, use_thumbnail=True, max_num=max_num)\n",
    "\n",
    "    # Transform and stack valid images\n",
    "    pixel_values = []\n",
    "    for img in images:\n",
    "        if isinstance(img, Image.Image):\n",
    "            pixel_values.append(transform(img))\n",
    "    return torch.stack(pixel_values)\n",
    "\n",
    "def split_model(model_name):\n",
    "    device_map = {}\n",
    "    world_size = torch.cuda.device_count()\n",
    "    config = AutoConfig.from_pretrained(model_path, trust_remote_code=True)\n",
    "    num_layers = config.llm_config.num_hidden_layers\n",
    "    # Since the first GPU will be used for ViT, treat it as half a GPU.\n",
    "    num_layers_per_gpu = math.ceil(num_layers / (world_size - 0.5))\n",
    "    num_layers_per_gpu = [num_layers_per_gpu] * world_size\n",
    "    num_layers_per_gpu[0] = math.ceil(num_layers_per_gpu[0] * 0.5)\n",
    "    layer_cnt = 0\n",
    "    for i, num_layer in enumerate(num_layers_per_gpu):\n",
    "        for j in range(num_layer):\n",
    "            device_map[f'language_model.model.layers.{layer_cnt}'] = i\n",
    "            layer_cnt += 1\n",
    "    device_map['vision_model'] = 0\n",
    "    device_map['mlp1'] = 0\n",
    "    device_map['language_model.model.tok_embeddings'] = 0\n",
    "    device_map['language_model.model.embed_tokens'] = 0\n",
    "    device_map['language_model.output'] = 0\n",
    "    device_map['language_model.model.norm'] = 0\n",
    "    device_map['language_model.model.rotary_emb'] = 0\n",
    "    device_map['language_model.lm_head'] = 0\n",
    "    device_map[f'language_model.model.layers.{num_layers - 1}'] = 0\n",
    "\n",
    "    return device_map\n",
    "\n",
    "# If you set `load_in_8bit=True`, you will need two 80GB GPUs.\n",
    "# If you set `load_in_8bit=False`, you will need at least three 80GB GPUs.\n",
    "model_path = 'OpenGVLab/InternVL3-8B'\n",
    "device_map = split_model('InternVL3-8B')\n",
    "model = AutoModel.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    load_in_8bit=False,\n",
    "    low_cpu_mem_usage=True,\n",
    "    use_flash_attn=False,\n",
    "    trust_remote_code=True,\n",
    "    device_map=device_map).eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True, use_fast=False)\n",
    "\n",
    "\n",
    "INTERN_VL3_CACHE_PATH = \"intern_vl3_cache.json\"\n",
    "INTERN_VL3_CACHE = {}\n",
    "\n",
    "# Manual save function\n",
    "def manual_save_intern_vl3():\n",
    "    with open(INTERN_VL3_CACHE_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(INTERN_VL3_CACHE, f, indent=2)\n",
    "\n",
    "# Load cache if it exists\n",
    "if os.path.exists(INTERN_VL3_CACHE_PATH):\n",
    "    with open(INTERN_VL3_CACHE_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "        INTERN_VL3_CACHE = json.load(f)\n",
    "\n",
    "# Auto-save every 10 requests\n",
    "intern_vl3_counter = 0\n",
    "def save_intern_vl3_cache():\n",
    "    global intern_vl3_counter\n",
    "    intern_vl3_counter += 1\n",
    "    if intern_vl3_counter % 10 == 0:\n",
    "        with open(INTERN_VL3_CACHE_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(INTERN_VL3_CACHE, f, indent=2)\n",
    "\n",
    "# Cache key generation\n",
    "def make_cache_key(before_image: Image.Image, after_image: Image.Image, prompt: str) -> str:\n",
    "    before_id = get_image_id(before_image)\n",
    "    after_id = get_image_id(after_image)\n",
    "    return f\"{before_id}_{after_id}_{hashlib.md5(prompt.encode('utf-8')).hexdigest()}\"\n",
    "\n",
    "# Prediction with caching\n",
    "@torch.no_grad()\n",
    "def get_intern_vl3_response(inputs):\n",
    "    before_image = inputs[0]\n",
    "    after_image = inputs[1]\n",
    "    prompt = inputs[2]\n",
    "    cache_key = make_cache_key(before_image, after_image, prompt)\n",
    "    if cache_key in INTERN_VL3_CACHE:\n",
    "        return INTERN_VL3_CACHE[cache_key]\n",
    "\n",
    "    print(\"[CACHE MISS] Running InternVL3...\")\n",
    "\n",
    "    pixel_values1 = load_image(before_image, max_num=12).to(torch.bfloat16).cuda()\n",
    "    pixel_values2 = load_image(after_image, max_num=12).to(torch.bfloat16).cuda()\n",
    "    pixel_values = torch.cat((pixel_values1, pixel_values2), dim=0)\n",
    "\n",
    "    response = model.chat(tokenizer, pixel_values, prompt, generation_config)\n",
    "    INTERN_VL3_CACHE[cache_key] = response\n",
    "    save_intern_vl3_cache()\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b679389-31b8-4636-8747-b6cede1f5524",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f9f3de-96ea-48cb-b680-c9bce84efdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = ['gemini', 'gpt-4', 'gpt-4o', 'gpt-4-turbo', 'gemini-1.5', 'pipeline']\n",
    "# MODELS = ['gpt-4', 'pipeline']\n",
    "\n",
    "def get_task_relevant_pipeline_column(task_name):\n",
    "    if task_name in ['is_extensive_difference_caption_accurate', 'is_difference_captipn_accurate', 'is_accurate']:\n",
    "        return 'is_edit_accurate'\n",
    "    elif task_name == 'is_artifacts':\n",
    "        return 'is_edit_contains_artifacts'\n",
    "    elif task_name == 'is_good_quality':\n",
    "        return 'is_bad_quality_edit'\n",
    "\n",
    "def get_human_prediction(example, task_name):\n",
    "    if task_name in ['is_difference_captions_same_complex', 'single difference caption']:\n",
    "        return get_selected_annotated_difference_caption(example, second_prioretiy=True)\n",
    "    raise Exception('Human evlauation not defined for task')\n",
    "\n",
    "    \n",
    "def get_pipeline_prediction(example, task_name):\n",
    "    if task_name in ['is_extensive_difference_caption_accurate', 'is_difference_captipn_accurate', 'is_accurate', 'is_artifacts']:\n",
    "        return 'Yes' if example[get_task_relevant_pipeline_column(task_name)] else 'No'\n",
    "    elif task_name == 'is_good_quality':\n",
    "        return 'No' if example[get_task_relevant_pipeline_column(task_name)] else 'Yes'\n",
    "    elif task_name in ['is_difference_captions_same_complex', 'single difference caption']:\n",
    "        return example['extensive_caption']\n",
    "    else:\n",
    "        raise Exception(task_name)\n",
    "\n",
    "def get_model_response(inputs, model, example, task_name):\n",
    "    if model == 'pipeline':\n",
    "        return get_pipeline_prediction(example, task_name)\n",
    "    elif model == 'gemini':\n",
    "        return get_geimini_response(inputs)\n",
    "    elif model == 'gemini-1.5':\n",
    "        return get_geimini_response(inputs, model=GEMINI_MODE_one_half)\n",
    "    elif model == 'intern-vl3':\n",
    "        return get_intern_vl3_response(inputs)\n",
    "    elif model == 'qwen':\n",
    "        return get_qwen_response(inputs)\n",
    "    elif model == 'gpt-4':\n",
    "        return get_gpt_4_vision_response(inputs)\n",
    "    elif model == 'gpt-4o':\n",
    "        return get_gpt_4_vision_response(inputs, MODEL='gpt-4o')\n",
    "    elif model == 'gpt-4-turbo':\n",
    "        return get_gpt_4_vision_response(inputs, MODEL='gpt-4-turbo')\n",
    "    elif model == 'human':\n",
    "        return get_human_prediction(example, task_name)\n",
    "    raise Exception('Model evaluation method not implemented')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e360b8-334f-4e38-982c-5c0cde56933f",
   "metadata": {},
   "source": [
    "### Difference Caption Evaluation Metrics & Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551ac5cf-effb-41cd-982e-e8e43109753a",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_general_edit_actions_from_discription = open(PROJECT_PATH + \"//prompts/generate_general_edit_actions_from_discription.txt\").read()\n",
    "general_detect_the_difference_prompt = open(PROJECT_PATH + '//prompts/detect_the_difference_prompt.txt', \"r\").read()\n",
    "get_objects_similarity_label_prompt = open(PROJECT_PATH + '//prompts/get_objects_similarity_label_prompt.txt', \"r\").read()\n",
    "caption_replacer = '$REPLACE_HERE$'\n",
    "\n",
    "identical_columns = {\"exact match {}\"}\n",
    "almost_identical_columns = {\n",
    "    \"exact match different order {}\", \n",
    "    \"partial match both source and target different action {}\"\n",
    "}\n",
    "completely_wrong = {\"same action different objects {}\", \"completely different {}\"}\n",
    "under_specify = completely_wrong.union({\n",
    "    \"partial match either source or target same action {}\", \n",
    "    \"partial match either source or target different action {}\", \n",
    "    \"partial match either source or target different order same action {}\", \n",
    "    \"partial match either source or target different order different action {}\"\n",
    "})\n",
    "all_metrics_columns = list(identical_columns.union(almost_identical_columns).union(under_specify).union(completely_wrong))\n",
    "\n",
    "def select_elements_by_indexes(my_list, indexes):\n",
    "    selected_elements = []\n",
    "    for i in indexes:\n",
    "        selected_elements.append(my_list[i])\n",
    "    return selected_elements\n",
    "\n",
    "def get_selected_annotated_difference_caption(example, second_prioretiy=False):\n",
    "    matching_indexes = list(range(0, len(example['metadata_annotated_difference_caption_explanation'])))\n",
    "    try:\n",
    "        gorund_truth_artifacts = example['annotated_is_artifacts']\n",
    "        gorund_truth_caption = example['annotated_is_extensive_difference_caption_accurate']\n",
    "        \n",
    "        artifacts_annotations = example['metadata_annotated_is_artifacts']\n",
    "        caption_annotations = example['metadata_annotated_is_extensive_difference_caption_accurate']\n",
    "        \n",
    "        if len(artifacts_annotations) != len(caption_annotations):\n",
    "            print(artifacts_annotations, type(artifacts_annotations))\n",
    "            print(caption_annotations, type(caption_annotations))\n",
    "            raise ValueError(\"Both arrays must have the same length\")\n",
    "    \n",
    "        # Find indexes where both arrays have the ground_truth value\n",
    "        matching_indexes = [i for i in range(len(artifacts_annotations)) if artifacts_annotations[i] == gorund_truth_artifacts and caption_annotations[i] == gorund_truth_caption]\n",
    "    except Exception as e:\n",
    "        print('get_selected_annotated_difference_caption - ' +  str(e))\n",
    "            \n",
    "    matching_descriptions = select_elements_by_indexes(example['metadata_annotated_difference_caption_explanation'], matching_indexes)\n",
    "    matching_descriptions = sorted(matching_descriptions, key=len, reverse=True)\n",
    "    if len(matching_descriptions) == 0 or len(matching_descriptions[0]) < 5: # In case of accurate edit\n",
    "        return example['extensive_caption']\n",
    "    if not second_prioretiy and 'unexpected' not in matching_descriptions[0]:\n",
    "        return matching_descriptions[0]\n",
    "    else:\n",
    "        try:\n",
    "            return matching_descriptions[1]\n",
    "        except:\n",
    "            captions = example['metadata_annotated_difference_caption_explanation'].copy()\n",
    "            captions.remove(matching_descriptions[0])\n",
    "            return captions[0] if len(captions[0]) > len(captions[1]) else captions[1]\n",
    "\n",
    "def get_general_difference_prompt(before_image, after_image, model, example, task):\n",
    "    return get_model_response([before_image, after_image, general_detect_the_difference_prompt], model=model, example=example, task_name=task)\n",
    "\n",
    "def get_action_id(edit_action, id_prefix, debug=False):\n",
    "    debug and print('Id fuction recived:,', edit_action)\n",
    "    return f'{id_prefix}{edit_action[\"action\"]}_{edit_action[\"source_object\"]}_{edit_action[\"target_object\"]}'\n",
    "\n",
    "def get_similarity_label(first_object, second_object, debug=False):\n",
    "    prompt = get_objects_similarity_label_prompt.format(first_object, second_object)\n",
    "    debug and print(prompt)\n",
    "    response = get_gpt_4_vision_response([prompt], MODEL='gpt-4o')\n",
    "    cleaned_response = response.replace('```json', '').replace('```', '').strip()\n",
    "    try:\n",
    "        parsed_json = json.loads(cleaned_response)\n",
    "        return parsed_json['similarityLevel']\n",
    "    except:\n",
    "        print('Recived invalid response', cleaned_response, 'original response', response)\n",
    "        return 0\n",
    "\n",
    "def get_edit_actions_from_description(description, id_prefix, debug=False):\n",
    "    response = None\n",
    "    if not is_nan_or_none(description):\n",
    "        prompt = generate_general_edit_actions_from_discription.replace(caption_replacer, description)\n",
    "        try:\n",
    "            debug and print('Edit action from description prompt: ', prompt)\n",
    "            response = get_chatgpt_4_prediction(prompt)\n",
    "            if response.replace('\\n', '').strip() == 'None':\n",
    "                parsed_response = []\n",
    "            else:\n",
    "                parsed_response = json.loads(response)\n",
    "            debug and print('Edit action from description response: ', parsed_response)\n",
    "            for action in parsed_response:\n",
    "                action.update({'id': get_action_id(action, id_prefix, debug=debug)})\n",
    "            return parsed_response\n",
    "        except:\n",
    "            debug and print(prompt, response)\n",
    "            print('Failed to get actions from description - debug here')\n",
    "            return []\n",
    "    else:\n",
    "        print('Recived NaN Difference caption (This is probably caption pipleine fail).')\n",
    "        print(description)\n",
    "        print(id_prefix)\n",
    "        return []\n",
    "        \n",
    "\n",
    "gpt_nouns = set()\n",
    "def get_action_similarity_level(first_object, second_object, debug=False):\n",
    "    global gpt_nouns\n",
    "    if first_object == second_object:\n",
    "        return '2'\n",
    "    first_object_words = list(filter(lambda word: is_noun(word), word_tokenize(first_object)))\n",
    "    second_object_words = list(filter(lambda word: is_noun(word), word_tokenize(second_object)))\n",
    "    \n",
    "    second_object_synonyms_nouns = []\n",
    "    for word in second_object_words:\n",
    "        second_object_synonyms_nouns.append(word)\n",
    "        second_object_synonyms_nouns = second_object_synonyms_nouns + get_related_synonyms(word)\n",
    "    \n",
    "    first_object_synonyms_nouns = []\n",
    "    for word in first_object_words:\n",
    "        first_object_synonyms_nouns.append(word)\n",
    "        first_object_synonyms_nouns = first_object_synonyms_nouns + get_related_synonyms(word)\n",
    "    \n",
    "    is_first_and_second_objects_share_nouns = set(first_object_synonyms_nouns).intersection(set(second_object_synonyms_nouns))\n",
    "    if is_first_and_second_objects_share_nouns:\n",
    "        similarity = get_similarity_label(first_object, second_object, debug)\n",
    "        if not is_first_and_second_objects_share_nouns and is_first_and_second_objects_share_subtype_nouns:\n",
    "            gpt_nouns.add(first_object + ' + ' + second_object + ' + ' + str(similarity))\n",
    "            \n",
    "        return similarity\n",
    "    else:\n",
    "        return '0'\n",
    "    \n",
    "def get_action_similarity_details(ground_truth_edit_action, predicted_edit_action):\n",
    "    is_actions_replace_and_change_attribute = (ground_truth_edit_action['action'] in ['Change Attribute', 'Replace'] and predicted_edit_action['action'] in ['Change Attribute', 'Replace'])\n",
    "    is_same_action = (ground_truth_edit_action['action'] == predicted_edit_action['action']) or is_actions_replace_and_change_attribute\n",
    "    source_similarity_level = get_action_similarity_level(ground_truth_edit_action['source_object'], predicted_edit_action['source_object'])\n",
    "    target_similarity_level = get_action_similarity_level(ground_truth_edit_action['target_object'], predicted_edit_action['target_object'])\n",
    "    source_ground_target_predicted_similarity_level = get_action_similarity_level(ground_truth_edit_action['source_object'], predicted_edit_action['target_object'])\n",
    "    target_ground_source_predicted_similarity_level = get_action_similarity_level(ground_truth_edit_action['target_object'], predicted_edit_action['source_object'])\n",
    "    return is_same_action, source_similarity_level, target_similarity_level, source_ground_target_predicted_similarity_level, target_ground_source_predicted_similarity_level\n",
    "\n",
    "def remove_action_by_id(actions_list, id):\n",
    "    for action in actions_list:\n",
    "        if action.get('predicted_id') == id:\n",
    "            actions_list.remove(action)\n",
    "    return actions_list\n",
    "\n",
    "def is_exact_match(is_same_action, is_source_similar, is_target_similar, ground_action):\n",
    "    return (is_source_similar or ground_action['action'] == 'Add') and (is_target_similar or ground_action['action'] == 'Remove') and is_same_action\n",
    "\n",
    "def is_exact_match_different_order(is_target_and_source_similar_different_order, is_same_action):\n",
    "    return is_target_and_source_similar_different_order and is_same_action\n",
    "\n",
    "def is_partial_match_both_source_and_target_different_action(is_source_similar, is_target_similar, is_same_action, ground_action):\n",
    "    return (is_source_similar or ground_action['action'] == 'Add') and (is_target_similar or ground_action['action'] == 'Remove') and not is_same_action\n",
    "\n",
    "def is_partial_match_either_source_or_target_same_action(is_source_similar, is_target_similar, is_same_action):\n",
    "    return (is_source_similar or is_target_similar) and is_same_action\n",
    "\n",
    "def is_partial_match_either_source_or_target_different_action(is_source_similar, is_target_similar, is_same_action):\n",
    "    return (is_source_similar or is_target_similar) and not is_same_action\n",
    "\n",
    "def is_partial_match_either_source_or_target_different_order_same_action(is_target_or_source_similar_different_order, is_same_action):\n",
    "    return is_target_or_source_similar_different_order and is_same_action\n",
    "\n",
    "def is_partial_match_either_source_or_target_different_order_different_action(is_target_or_source_similar_different_order, is_same_action):\n",
    "    return is_target_or_source_similar_different_order and not is_same_action\n",
    "\n",
    "same_source_and_target_action = []\n",
    "def evaluate_actions_similarity(ground_truth_actions, predicted_actions, threshold):\n",
    "    \"\"\"Evaluate the similarity between ground truth actions and predicted actions.\"\"\"\n",
    "    similarity_categories = {\n",
    "        'exact match': [],\n",
    "        'exact match different order': [],\n",
    "        'partial match either source or target same action': [],\n",
    "        'partial match both source and target different action': [],\n",
    "        'partial match either source or target different action': [],\n",
    "        'partial match either source or target different order same action': [],\n",
    "        'partial match either source or target different order different action': [],\n",
    "        'same action different objects': [],\n",
    "        'completely different': [\n",
    "            {'predicted_id': action['id'], 'predicted_source': action['source_object'], 'predicted_target': action['target_object']}\n",
    "            for action in predicted_actions\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    for ground_action in ground_truth_actions:\n",
    "        for predicted_action in predicted_actions:\n",
    "            ids = [{\n",
    "                'ground_id': ground_action['id'], 'predicted_id': predicted_action['id'],\n",
    "                'ground_source': ground_action['source_object'], 'ground_target': ground_action['target_object'],\n",
    "                'predicted_source': predicted_action['source_object'], 'predicted_target': predicted_action['target_object']\n",
    "            }]\n",
    "            \n",
    "            is_same_action, source_similarity_level, target_similarity_level, source_ground_target_predicted_similarity_level, target_ground_source_predicted_similarity_level = get_action_similarity_details(ground_action, predicted_action)\n",
    "            \n",
    "            is_source_similar = int(source_similarity_level) >= threshold and ground_action['action'] != 'Add'\n",
    "            is_target_similar = int(target_similarity_level) >= threshold and ground_action['action'] != 'Remove'\n",
    "            is_target_or_source_similar_different_order = (\n",
    "                (int(source_ground_target_predicted_similarity_level) >= threshold and ground_action['action'] != 'Add') or \n",
    "                (int(target_ground_source_predicted_similarity_level) >= threshold and ground_action['action'] != 'Remove')\n",
    "            )\n",
    "            is_target_and_source_similar_different_order = (\n",
    "                int(source_ground_target_predicted_similarity_level) >= threshold and \n",
    "                int(target_ground_source_predicted_similarity_level) >= threshold\n",
    "            )\n",
    "\n",
    "            if any([is_same_action, is_source_similar, is_target_similar, is_target_or_source_similar_different_order]):\n",
    "                remove_action_by_id(similarity_categories['completely different'], predicted_action['id'])\n",
    "            # metadata_partial match either source or target same action gemini \n",
    "            if is_exact_match(is_same_action, is_source_similar, is_target_similar, ground_action):\n",
    "                similarity_categories['exact match'].extend(ids)\n",
    "            elif is_exact_match_different_order(is_target_and_source_similar_different_order, is_same_action):\n",
    "                similarity_categories['exact match different order'].extend(ids)\n",
    "            elif is_partial_match_both_source_and_target_different_action(is_source_similar, is_target_similar, is_same_action, ground_action):\n",
    "                similarity_categories['partial match both source and target different action'].extend(ids)\n",
    "            elif is_partial_match_either_source_or_target_same_action(is_source_similar, is_target_similar, is_same_action):\n",
    "                similarity_categories['partial match either source or target same action'].extend(ids)\n",
    "            elif is_partial_match_either_source_or_target_different_action(is_source_similar, is_target_similar, is_same_action):\n",
    "                similarity_categories['partial match either source or target different action'].extend(ids)\n",
    "            elif is_partial_match_either_source_or_target_different_order_same_action(is_target_or_source_similar_different_order, is_same_action):\n",
    "                similarity_categories['partial match either source or target different order same action'].extend(ids)\n",
    "            elif is_partial_match_either_source_or_target_different_order_different_action(is_target_or_source_similar_different_order, is_same_action):\n",
    "                similarity_categories['partial match either source or target different order different action'].extend(ids)\n",
    "            elif is_same_action:\n",
    "                similarity_categories['same action different objects'].extend(ids)\n",
    "\n",
    "    return tuple(similarity_categories[key] for key in similarity_categories)\n",
    "\n",
    "def enrich_with_evaluation_data(example, ground_truth_difference_caption, predicted_difference_caption, model, threshold=1):\n",
    "    global same_source_and_target_action\n",
    "    \"\"\"Enrich the example with evaluation data based on predicted and ground truth captions.\"\"\"\n",
    "    if example.get('ground truth edit actions') is None:\n",
    "        example['ground truth edit actions'] = get_edit_actions_from_description(ground_truth_difference_caption, 'ground truth_')\n",
    "\n",
    "    ground_truth_edit_actions = example['ground truth edit actions']\n",
    "    predicted_edit_actions = get_edit_actions_from_description(predicted_difference_caption, f'{model}_')\n",
    "    example[f'predicted edit actions {model}'] = predicted_edit_actions\n",
    "    \n",
    "    (exact_match, exact_match_different_order, partial_match_either_source_or_target_same_action, \n",
    "     partial_match_both_source_and_target_different_action, partial_match_either_source_or_target_different_action, \n",
    "     partial_match_either_source_or_target_different_order_same_action, partial_match_either_source_or_target_different_order_different_action, \n",
    "     same_action_different_objects, completely_different) = evaluate_actions_similarity(ground_truth_edit_actions, predicted_edit_actions, threshold)\n",
    "  \n",
    "    same_source_and_target_action.extend(partial_match_both_source_and_target_different_action)\n",
    "    example[f'exact match {model}'] = exact_match\n",
    "    example[f'exact match different order {model}'] = exact_match_different_order\n",
    "    example[f'partial match either source or target same action {model}'] = partial_match_either_source_or_target_same_action\n",
    "    example[f'partial match both source and target different action {model}'] = partial_match_both_source_and_target_different_action\n",
    "    example[f'partial match either source or target different action {model}'] = partial_match_either_source_or_target_different_action\n",
    "    example[f'partial match either source or target different order same action {model}'] = partial_match_either_source_or_target_different_order_same_action\n",
    "    example[f'partial match either source or target different order different action {model}'] = partial_match_either_source_or_target_different_order_different_action\n",
    "    example[f'same action different objects {model}'] = same_action_different_objects\n",
    "    example[f'completely different {model}'] = completely_different\n",
    "    \n",
    "    return example\n",
    "\n",
    "    \n",
    "def enrich_free_text_difference_caption_evaluation_details(example, threshold=1, boolean_evaluation_mode=False):\n",
    "    example = load_example_images(example)\n",
    "    example['eval_selected_difference_caption'] = get_selected_annotated_difference_caption(example)\n",
    "    for model in MODELS:\n",
    "        predicted_difference_prompt = get_general_difference_prompt(example['source_img'], example['target_img'], model, example, 'is_difference_captions_same_complex')\n",
    "        example[f'predicted_difference_caption_{model}'] = predicted_difference_prompt\n",
    "        if not boolean_evaluation_mode:\n",
    "            enrich_with_evaluation_data(example, example['eval_selected_difference_caption'], predicted_difference_prompt, model, threshold=threshold)\n",
    "    return example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53dbac80-9209-41b3-a246-52f35d36bf06",
   "metadata": {},
   "source": [
    "#### Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naughty-sweden",
   "metadata": {},
   "source": [
    "##### Generation Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8dfd244-8eee-4488-b05e-de18a166dc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from tqdm import tqdm\n",
    "from pandas.errors import PerformanceWarning\n",
    "is_diffrence_captions_main_diffrence_same = 'Determine if the primary difference is accurately present in the caption. Answer \"Yes\" or \"No\" only.\\n\\nPrimary difference:\\n{}\\n\\nCaption:\\n{}\\n\\nQuestion:\\nIs the primary difference present in the caption? (Answer Yes/No only)'\n",
    "is_diffrence_captions_same_prompt = open(PROJECT_PATH + '//prompts/is_diffrence_captions_same.txt', \"r\").read()\n",
    "get_main_diffrence_prompt = open(PROJECT_PATH + '//prompts/get_main_difference_prompt.txt', \"r\").read()\n",
    "generate_single_change_difference_caption_prompt = 'Please describe the main difference between the two images.'\n",
    "get_visual_consistency_prompt = open(PROJECT_PATH + '//prompts/get_visual_consistency_prompt.txt', 'r').read()\n",
    "get_visual_quality_prompt = open(PROJECT_PATH + '//prompts/get_visual_quality_prompt.txt', 'r').read()\n",
    "is_feedback_same_prompt = open(PROJECT_PATH + '//prompts/is_feedback_same_prompt.txt', 'r').read()\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, message=\"Setting an item of incompatible dtype is deprecated\")\n",
    "warnings.simplefilter(action='ignore', category=PerformanceWarning)\n",
    "\n",
    "def get_columns_actions_statistics(example, model, cluster_columns, debug=False):\n",
    "    # Format the selected columns for the model\n",
    "    selected_columns = [column.format(model) for column in cluster_columns]\n",
    "    if debug:\n",
    "        print('Selected columns -', selected_columns)\n",
    "    \n",
    "    # Get ground truth IDs\n",
    "    ground_truth_ids = {action['id'] for action in example['ground truth edit actions']}\n",
    "    \n",
    "    # Collect ground truth IDs from the selected columns\n",
    "    selected_columns_ground_truth_ids = set()\n",
    "    selected_columns_predicted_ids = set()\n",
    "    for column in selected_columns:\n",
    "        selected_columns_ground_truth_ids.update(action['ground_id'] for action in example[f'metadata_{column}'])\n",
    "        selected_columns_predicted_ids.update(action['predicted_id'] for action in example[f'metadata_{column}'])\n",
    "    \n",
    "    # Ensure selected column IDs do not exceed ground truth IDs\n",
    "    assert len(selected_columns_ground_truth_ids) <= len(ground_truth_ids)\n",
    "    if debug:\n",
    "        print('Ground Truth Ids:', ground_truth_ids)\n",
    "        print('Ids in predictions:', selected_columns_ground_truth_ids)\n",
    "        print('# of misses:', len(ground_truth_ids - selected_columns_ground_truth_ids))\n",
    "    \n",
    "    # Calculate identical IDs and percentages\n",
    "    identical_ids = ground_truth_ids - selected_columns_ground_truth_ids\n",
    "    percentage_of_ground_truth_identical_actions = len(selected_columns_ground_truth_ids) / len(ground_truth_ids)\n",
    "    predicted_edit_actions_count = len(example[f'predicted edit actions {model}'])\n",
    "    percentage_of_predicted_identical_actions = (len(selected_columns_predicted_ids) / predicted_edit_actions_count) if predicted_edit_actions_count != 0 else 0\n",
    "\n",
    "    # Return results\n",
    "    if not is_nan_or_none(percentage_of_ground_truth_identical_actions):\n",
    "        return len(identical_ids) == 0, percentage_of_ground_truth_identical_actions, percentage_of_predicted_identical_actions\n",
    "    else:\n",
    "        return len(identical_ids) == 0, 0, 0\n",
    "\n",
    "def is_over(row, model):\n",
    "    if get_columns_actions_statistics(row, model, almost_identical_columns)[0] or get_columns_actions_statistics(row, model, identical_columns)[0]:\n",
    "        under = False\n",
    "        for col in all_metrics_columns:\n",
    "            if col in under_specify:\n",
    "                if row[col.format(model)] > 0:\n",
    "                    under = True\n",
    "        return under\n",
    "    return False    \n",
    "\n",
    "def is_under(row, model):\n",
    "    if not get_columns_actions_statistics(row, model, almost_identical_columns)[0] and not get_columns_actions_statistics(row, model, identical_columns)[0]:\n",
    "        under = False\n",
    "        for col in all_metrics_columns:\n",
    "            if col in under_specify:\n",
    "                if row[col.format(model)] > 0:\n",
    "                    under = True\n",
    "        return under\n",
    "    return False\n",
    "\n",
    "def get_example_main_difference(example, debug=False):\n",
    "    example_explenation = example['metadata_annotated_difference_caption_explanation']\n",
    "    prompt = get_main_diffrence_prompt.format(example_explenation[0], example_explenation[1], example_explenation[2])\n",
    "    response = get_model_response([prompt], model='gpt-4o', example=example, task_name='main difference')\n",
    "    debug and print(prompt)\n",
    "    debug and print(response)\n",
    "    return response\n",
    "\n",
    "def generate_single_change_difference_caption(row, model, debug=False):\n",
    "    return get_model_response([row['source_img'], row['target_img'], generate_single_change_difference_caption_prompt], model=model, example=row, task_name='single difference caption')\n",
    "    \n",
    "def find_longest_string(arr):\n",
    "    if not arr:  # Check if the array is empty\n",
    "        return None\n",
    "    return max(arr, key=len)\n",
    "\n",
    "def get_is_visual_consistency_feedback_same(row, model, debug=False):\n",
    "    row['eval_visualy_consistence_explanation'] = find_longest_string(row['metadata_annotated_visualy_consistence_explanation'])\n",
    "    debug and print('Ground truth visual consistency response: \\n', row['eval_visualy_consistence_explanation'] )\n",
    "    row['predicted_visualy_consistence_explanation_{}'.format(model)] = get_model_response([row['source_img'], row['target_img'], get_visual_consistency_prompt.format(row['Instruction'])], model=model, example=row, task_name='feedback')\n",
    "    debug and print('Model visual consistency response: \\n', row['predicted_visualy_consistence_explanation_{}'.format(model)])\n",
    "    prompt = is_feedback_same_prompt.format(row['eval_visualy_consistence_explanation'], row['predicted_visualy_consistence_explanation_{}'.format(model)])\n",
    "    response = get_model_response([prompt], model='gpt-4o', example=row, task_name='is_feedback_same_boolean')\n",
    "    debug and print(response, '\\n')\n",
    "    return smart_convert_yes_no_list([response])[0], response\n",
    "\n",
    "def get_is_technical_precision_feedback_same(row, model, debug=False):\n",
    "    row['eval_good_quality_explanation'] = find_longest_string(row['metadata_annotated_good_quality_explanation'])\n",
    "    debug and print('Ground truth good visual response: \\n', row['eval_good_quality_explanation'] )\n",
    "    row['predicted_good_quality_explanation_{}'.format(model)] = get_model_response([row['source_img'], row['target_img'], get_visual_quality_prompt.format(row['Instruction'])], model=model, example=row, task_name='feedback')\n",
    "    debug and print('Model good visual response: \\n', row['predicted_good_quality_explanation_{}'.format(model)])\n",
    "    prompt = is_feedback_same_prompt.format(row['eval_good_quality_explanation'], row['predicted_good_quality_explanation_{}'.format(model)])\n",
    "    response = get_model_response([prompt], model='gpt-4o', example=row, task_name='is_feedback_same_boolean')\n",
    "    debug and print(response, '\\n')\n",
    "    return smart_convert_yes_no_list([response])[0], response\n",
    "\n",
    "def get_is_difference_captions_same(row, model_difference_caption, single_change=False, debug=False):\n",
    "    if single_change:\n",
    "        prompt = is_diffrence_captions_main_diffrence_same.format(row['caption_primary_difference'], model_difference_caption)\n",
    "    else:\n",
    "        ground_truth_difference_caption  = row['eval_selected_difference_caption']\n",
    "        prompt = is_diffrence_captions_same_prompt.format(ground_truth_difference_caption, model_difference_caption)\n",
    "    response = get_model_response([prompt], model='gpt-4o', example=row, task_name='is_difference_captions_same_boolean')\n",
    "    debug and print(prompt)\n",
    "    debug and print(response)\n",
    "    return smart_convert_yes_no_list([response])[0], response\n",
    "\n",
    "def initalize_model_stats(stats, model, lingustic_metrics, boolean_evaluation_mode):\n",
    "    stats.update({f'is_difference_caption_same_{model}_': 0, f'is_ALL_difference_caption_main_difference_same_{model}_': 0})\n",
    "    stats.update({f'is_single_change_difference_caption_same_{model}_': 0})\n",
    "    stats.update({f'is_visualy_consistence_feedback_same_{model}_': 0, f'is_technical_precision_feedback_same_{model}_': 0})\n",
    "    if lingustic_metrics:\n",
    "        stats.update({f'meteor_{model}_': 0, f'rouge1_recall_{model}_': 0, f'sentece_similarity_{model}_': 0 })\n",
    "    if not boolean_evaluation_mode:\n",
    "        stats.update({f'almost_identical_{model}_': 0, f'under_{model}_': 0, f'over_{model}_': 0, f'identical_{model}_': 0, f'almost_identical_ground_truth_actions_{model}_': 0, f'identical_ground_truth_actions_{model}_': 0, f'almost_identical_predicted_actions_{model}_': 0, f'identical_predicted_actions_{model}_': 0,  })\n",
    "    return stats\n",
    "\n",
    "def update_stats(stats, column, model, index, value, evaluation_df_parsed):\n",
    "    stats.update({f'{column}_{model}_': stats.get(f'{column}_{model}_') + value})\n",
    "    evaluation_df_parsed.at[index, f'{column}_{model}_'] = value\n",
    "\n",
    "def evaluate_models(evaluation_df, boolean_evaluation_mode=False, lingustic_metrics=False, evaluate_consistency_and_precision=False, evaluate_all_difference_caption=False):\n",
    "    stats = dict()\n",
    "    metadata = []\n",
    "\n",
    "    evaluation_df_parsed = evaluation_df.copy()\n",
    "    if not boolean_evaluation_mode:\n",
    "        for model in MODELS:\n",
    "            model_base_metrics_columns = list(map(lambda x: x.format(model), all_metrics_columns))\n",
    "            metadata_model_base_metrics_columns = list(map(lambda x: 'metadata_' + x.format(model), all_metrics_columns))\n",
    "            evaluation_df_parsed[metadata_model_base_metrics_columns] = evaluation_df[model_base_metrics_columns]\n",
    "            evaluation_df_parsed[model_base_metrics_columns] = evaluation_df[model_base_metrics_columns].map(len)\n",
    "    \n",
    "    for model in MODELS:\n",
    "        stats = initalize_model_stats(stats, model, lingustic_metrics, boolean_evaluation_mode)   \n",
    "        technical_precision_counter = 0   \n",
    "        contextual_consistency_counter = 0    \n",
    "        for index, row in tqdm(evaluation_df_parsed.iterrows(), total=len(evaluation_df_parsed), desc=f\"{model} predictions\"):\n",
    "            row = load_example_images(row)\n",
    "            evaluation_df_parsed.at[index, 'caption_primary_difference'] = row['caption_primary_difference'] = get_example_main_difference(row)\n",
    "            row_metadata = dict()\n",
    "            \n",
    "            if evaluate_all_difference_caption:\n",
    "                # Difference caption - main difference\n",
    "                is_difference_caption_same, response = get_is_difference_captions_same(row, row['predicted_difference_caption_{}'.format(model)])\n",
    "                is_difference_caption_main_difference_same, response_main_diffrence = get_is_difference_captions_same(row, row['predicted_difference_caption_{}'.format(model)], single_change=True)\n",
    "                row_metadata.update({'model': model, 'id': row['id'],'is_difference_caption_same': is_difference_caption_same, 'is_difference_caption_same_response': response,  'is_difference_caption_main_difference_same': is_difference_caption_main_difference_same, 'is_ALL_difference_caption_main_difference_same_response': response_main_diffrence})\n",
    "            \n",
    "                            \n",
    "                # This is the main difference of the big all difference captions of models similarity\n",
    "                if is_difference_caption_main_difference_same:\n",
    "                    stats.update({f'is_ALL_difference_caption_main_difference_same_{model}_': stats.get(f'is_ALL_difference_caption_main_difference_same_{model}_') + 1})\n",
    "                    evaluation_df_parsed.at[index, f'is_ALL_difference_caption_main_difference_same_{model}_'] = True\n",
    "                    \n",
    "                if is_difference_caption_same:\n",
    "                    stats.update({f'is_difference_caption_same_{model}_': stats.get(f'is_difference_caption_same_{model}_') + 1})\n",
    "                    evaluation_df_parsed.at[index, f'is_difference_caption_same_{model}_'] = True\n",
    "                    \n",
    "            # Single Difference caption - main difference - is_single_change_difference_caption_main_difference_same_\n",
    "            single_change_difference_caption = generate_single_change_difference_caption(row, model) # genera a single difference caption for a speicific model\n",
    "            evaluation_df_parsed.at[index, 'predicted_single_change_difference_caption_{}'.format(model)] = single_change_difference_caption\n",
    "            is_single_change_difference_caption_same, response_is_single_change_difference_caption = get_is_difference_captions_same(row, single_change_difference_caption, single_change=True)\n",
    "            row_metadata.update({'single_change_difference_caption': single_change_difference_caption, 'is_single_change_difference_caption_same': is_single_change_difference_caption_same, 'response_is_single_change_difference_caption': response_is_single_change_difference_caption})\n",
    "            \n",
    "            if evaluate_consistency_and_precision:\n",
    "                if not row['annotated_is_good_quality'] and model not in ['pipeline', 'human'] and 'finetune' not in model:\n",
    "                    technical_precision_counter += 1\n",
    "                    is_technical_precision_feedback_same = get_is_technical_precision_feedback_same(row, model)[0]\n",
    "                    row_metadata.update({'is_technical_precision_feedback_same_': is_technical_precision_feedback_same})\n",
    "                    if is_technical_precision_feedback_same:\n",
    "                        stats.update({f'is_technical_precision_feedback_same_{model}_': stats.get(f'is_technical_precision_feedback_same_{model}_') + 1})\n",
    "                if not row['annotated_is_visualy_consistence'] and model not in ['pipeline', 'human'] and 'finetune' not in model:\n",
    "                    contextual_consistency_counter += 1\n",
    "                    is_visual_consistency_feedback_same = get_is_visual_consistency_feedback_same(row, model)[0]\n",
    "                    row_metadata.update({'is_visualy_consistence_feedback_same': is_visual_consistency_feedback_same})\n",
    "                    if is_visual_consistency_feedback_same:\n",
    "                        stats.update({f'is_visualy_consistence_feedback_same_{model}_': stats.get(f'is_visualy_consistence_feedback_same_{model}_') + 1})\n",
    "                \n",
    "                meteor, rouge1_recall, sentece_similarity = None, None, None\n",
    "                if lingustic_metrics:\n",
    "                    model_predicted_difference_caption = row['predicted_difference_caption_{}'.format(model)]\n",
    "                    if isinstance(model_predicted_difference_caption, str):\n",
    "                        meteor, rouge1_recall, sentece_similarity = evaluate_captions(model_predicted_difference_caption, model_predicted_difference_caption)\n",
    "\n",
    "                if lingustic_metrics and meteor and isinstance(model_predicted_difference_caption, str):\n",
    "                    row_metadata.update({'meteor': meteor, 'rouge1_recall': rouge1_recall, 'sentece_similarity': sentece_similarity})\n",
    "                    stats.update({f'sentece_similarity_{model}_': stats.get(f'sentece_similarity_{model}_') + sentece_similarity})\n",
    "                    stats.update({f'rouge1_recall_{model}_': stats.get(f'rouge1_recall_{model}_') + rouge1_recall})\n",
    "                    stats.update({f'meteor_{model}_': stats.get(f'meteor_{model}_') + meteor})\n",
    "            \n",
    "            # This is predicting the a short difference caption is it similar to the main difference extracted from human anntations\n",
    "            if is_single_change_difference_caption_same:\n",
    "                stats.update({f'is_single_change_difference_caption_same_{model}_': stats.get(f'is_single_change_difference_caption_same_{model}_') + 1})\n",
    "                evaluation_df_parsed.at[index, f'is_single_change_difference_caption_same_{model}_'] = True\n",
    "                \n",
    "            if boolean_evaluation_mode:\n",
    "                continue\n",
    "\n",
    "            identical, identical_ground_truth_precentage, identical_predicted_precentage = get_columns_actions_statistics(row, model, identical_columns)\n",
    "            almost_identical, almost_identical_ground_truth_precentage, almost_identical_predicted_precentage = get_columns_actions_statistics(row, model, almost_identical_columns)\n",
    "            under, over = is_under(row, model), is_over(row, model)\n",
    "            \n",
    "            row_metadata.update({'index': index, 'model': model, 'identical': identical, 'almost_identical': almost_identical, 'under': under, 'over': over})\n",
    "            metadata.append(row_metadata)\n",
    "            number_of_categories = sum([almost_identical, under, over, identical])\n",
    "            # assert(number_of_categories <= 1)\n",
    "            \n",
    "            if identical and not over:\n",
    "                stats.update({f'identical_{model}_': stats.get(f'identical_{model}_') + 1})\n",
    "                evaluation_df_parsed.at[index, f'identical_{model}_'] = True\n",
    "            \n",
    "            if almost_identical:\n",
    "                stats.update({f'almost_identical_{model}_': stats.get(f'almost_identical_{model}_') + 1})\n",
    "                evaluation_df_parsed.at[index, f'almost_identical_{model}_'] = True\n",
    "            \n",
    "            if under or number_of_categories == 0:\n",
    "                stats.update({f'under_{model}_': stats.get(f'under_{model}_') + 1})\n",
    "                evaluation_df_parsed.at[index, f'under_{model}_'] = True\n",
    "            \n",
    "            if over:\n",
    "                stats.update({f'over_{model}_': stats.get(f'over_{model}_') + 1})\n",
    "                evaluation_df_parsed.at[index, f'over_{model}_'] = True\n",
    "            \n",
    "            update_stats(stats, 'identical_ground_truth_actions', model, index, identical_ground_truth_precentage, evaluation_df_parsed)\n",
    "            update_stats(stats, 'almost_identical_ground_truth_actions', model, index, almost_identical_ground_truth_precentage, evaluation_df_parsed)\n",
    "            update_stats(stats, 'identical_predicted_actions', model, index, identical_predicted_precentage, evaluation_df_parsed)\n",
    "            update_stats(stats, 'almost_identical_predicted_actions', model, index, almost_identical_predicted_precentage, evaluation_df_parsed)\n",
    "    \n",
    "        original_stats = stats.copy()\n",
    "        updated_stats = {}  # New dictionary to hold the updated values\n",
    "\n",
    "        for key in original_stats:\n",
    "            if f'_{model}_' not in key:\n",
    "                continue\n",
    "            if 'visualy_consistence_feedback' in key:\n",
    "                updated_stats[key + '_precentage'] = round(original_stats.get(key)/(contextual_consistency_counter or 1), 2) * 100\n",
    "            elif 'technical_precision_feedback' in key:\n",
    "                updated_stats[key + '_precentage'] = round(original_stats.get(key)/(technical_precision_counter or 1), 2) * 100\n",
    "            elif 'precentage' not in key:\n",
    "                updated_stats[key + '_precentage'] = round(original_stats.get(key)/len(evaluation_df), 2) * 100\n",
    "\n",
    "        # Merge the new percentages back into the original stats\n",
    "        stats.update(updated_stats)\n",
    "\n",
    "    # Create a DataFrame for each model and concatenate them\n",
    "    dfs = []\n",
    "    for model in MODELS:\n",
    "        model_data = {k.replace(f'_{model}_', ''): v for k, v in stats.items() if f'_{model}_' in k}\n",
    "        for column_to_pop in ['meteor', 'rouge1_recall', 'sentece_similarity', f'almost_identical_actions', f'identical_actions']:\n",
    "            model_data.pop(column_to_pop, None)\n",
    "        df_model = pd.DataFrame([model_data], index=[model])\n",
    "        dfs.append(df_model)\n",
    "    \n",
    "    return pd.concat(dfs), evaluation_df_parsed, metadata, stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e22d2b",
   "metadata": {},
   "source": [
    "#### Run Difference Caption Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "intermediate-falls",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "LOCAL_RUN = False # Uses cached responsed only and will through error when trying to do an API call\n",
    "BOOLEAN_EVALUATION_MODE = False # Evaluate all tasks other then ALL difference caption task\n",
    "LINGUISTIC_METRICS_MODE = False # Rouge, Meteor, etc.\n",
    "EVALUATE_CONSISTENCY_AND_PRECISION = False # evaluate Visual Consistey and Techincal Precision\n",
    "EVALUATE_ALL_DIFFERENCE_CAPTION = True # Evaluate ALL difference caption task\n",
    "\n",
    "MODELS = ['intern-vl3', 'gpt-4o', 'gpt-4o', 'gemini-1.5', 'gpt-4','gpt-4-turbo', 'qwen']\n",
    "annotated_data = get_annotated_data_with_majority()\n",
    "\n",
    "enriched_difference_captions = annotated_data\n",
    "if EVALUATE_ALL_DIFFERENCE_CAPTION:\n",
    "    enriched_difference_captions = annotated_data.progress_apply(lambda x: enrich_free_text_difference_caption_evaluation_details(x, boolean_evaluation_mode=BOOLEAN_EVALUATION_MODE, threshold=1), axis=1)\n",
    "results_df_diff, evaluation_df_parsed_diff, metadata_diff, stats_diff = evaluate_models(enriched_difference_captions, boolean_evaluation_mode=BOOLEAN_EVALUATION_MODE, lingustic_metrics=LINGUISTIC_METRICS_MODE, evaluate_consistency_and_precision=EVALUATE_CONSISTENCY_AND_PRECISION, evaluate_all_difference_caption=EVALUATE_ALL_DIFFERENCE_CAPTION)\n",
    "results_df_diff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87055b90-78df-4d3f-bfc9-8bdabfe8f564",
   "metadata": {},
   "source": [
    "### Evaluation - Booleans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5beddb-7f70-4e8d-9820-c354b0cc5a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_introduction_prompt(example):\n",
    "    return f'You are provided with before and after images of an image edit for the edit instruction \"{example[\"instruction\"]}\".'\n",
    "\n",
    "def get_extensive_difference_caption_task_prompt(example, finetune_prompt=False):\n",
    "    if finetune_prompt:\n",
    "        return f'Does the difference caption \"{example[\"extensive_caption\"]}\" describes the difference between the two images (Answer only Yes/No)?'\n",
    "    else:\n",
    "        introduction = get_introduction_prompt(example)\n",
    "        return f'{introduction} Does the difference caption \"{example[\"extensive_caption\"]}\" describe the main difference between the two images reflected from the edit instruction (Answer only Yes/No)?'\n",
    "\n",
    "def get_difference_caption_task_prompt(example, finetune_prompt=False):\n",
    "    if finetune_prompt:\n",
    "        return f'Does the difference caption \"{example[\"extensive_caption\"]}\" describes the difference between the two images (Answer only Yes/No)?'\n",
    "    else:\n",
    "        introduction = get_introduction_prompt(example)\n",
    "        return f'{introduction} Does the difference caption \"{example[\"caption\"]}\" describe the main difference between the two images reflected from the edit instruction (Answer only Yes/No)?'\n",
    "\n",
    "def get_is_visualy_consistence_task_prompt(example, finetune_prompt=False):\n",
    "    introduction = get_introduction_prompt(example)\n",
    "    return f'{introduction} Is the edited object or the area affected by the instruction consistent with the edit instruction and the image scene in terms of shape, size, brightness, shadows, texture, color, etc. (Answer only Yes/No)?'\n",
    "\n",
    "def get_is_accurate_task_prompt(example, finetune_prompt=False):\n",
    "    if finetune_prompt:\n",
    "        return f'Did the edit instruction \"{example[\"Instruction\"]}\" was accurately executed and reflect the intended change (Answer only Yes/No)?'\n",
    "    else: \n",
    "        introduction = get_introduction_prompt(example)\n",
    "        return f'{introduction} Was the instruction accurately executed, and does it reflect the intended change? Ignore any other changes that do not relate to the instruction. Disregard visual quality issues like low resolution, blur, or unexpected properties such as shape, size, and color. (Answer Yes/No only)'\n",
    "\n",
    "def get_is_artifacts_caption_task_prompt(example, finetune_prompt=False):\n",
    "    if finetune_prompt:\n",
    "        return f'Are there any artifacts or alterations in the image not intended to be affected by the edit \"{example[\"instruction\"]}\" (Answer only Yes/No)?'\n",
    "    else:\n",
    "        introduction = get_introduction_prompt(example)\n",
    "        return f'{introduction} Are there any artifacts or alterations in the image not intended to be affected by the edit instruction (Answer only Yes/No)?'\n",
    "\n",
    "def get_is_good_quality_task_prompt(example, finetune_prompt=False):\n",
    "    introduction = get_introduction_prompt(example)\n",
    "    return f'Does the edited object or the area affected by the instruction \"{example[\"instruction\"]}\" maintain the image resolution, exhibit blur, show any smoothness, etc. (Answer only Yes/No)?'\n",
    "\n",
    "\n",
    "task_map = {\n",
    "    'is_extensive_difference_caption_accurate': get_extensive_difference_caption_task_prompt,\n",
    "    'is_difference_captipn_accurate': get_difference_caption_task_prompt,\n",
    "    'is_accurate': get_is_accurate_task_prompt,\n",
    "    'is_artifacts': get_is_artifacts_caption_task_prompt,\n",
    "    'is_visualy_consistence': get_is_visualy_consistence_task_prompt,\n",
    "    'is_good_quality': get_is_good_quality_task_prompt\n",
    "}\n",
    "\n",
    "# assert len(set(task_map.keys()).intersection(set(evaluation_columns))) == 5\n",
    "\n",
    "def enrich_with_task_details(data_df, predictions_output, task_map=task_map):\n",
    "    def enrich_models_predictions(example):\n",
    "        for task_name in task_map.keys():\n",
    "            task_prompt_function = task_map[task_name]\n",
    "            for model in MODELS:\n",
    "                task_prompt = task_prompt_function(example, finetune_prompt='finetune' in model)\n",
    "                example[model + f'_{task_name}_prediction'] = get_model_response([example['source_img'], example['target_img'], task_prompt], model=model, example=example, task_name=task_name)\n",
    "                \n",
    "        return example\n",
    "    \n",
    "    data_df = data_df.progress_apply(lambda x: enrich_models_predictions(x), axis=1)\n",
    "    if predictions_output is not None:\n",
    "        data_df.to_csv(predictions_output)\n",
    "        print('Saved predictions to ' + predictions_output)\n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb4da4e-66e3-4239-b6e9-c89d7566768e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.metrics import precision_score, f1_score, recall_score, balanced_accuracy_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Initialize tqdm for progress bar\n",
    "tqdm.pandas()\n",
    "\n",
    "# Define your evaluation function\n",
    "def evaluate_models(dataframe):\n",
    "    # Initialize dictionaries to store results for each metric\n",
    "    precision_results = {}\n",
    "    recall_results = {}\n",
    "    f1_results = {}\n",
    "    weighted_f1_results = {}\n",
    "    balanced_accuracy_results = {}\n",
    "    specificity_results = {}\n",
    "\n",
    "    # Identify annotated columns and model predictions\n",
    "    annotated_columns = [col for col in dataframe.columns if col.startswith(\"annotated_\")]\n",
    "    prediction_columns = [col for col in dataframe.columns if \"_prediction\" in col]\n",
    "    # Iterate through each annotated column and calculate metrics\n",
    "    for annotated_col in annotated_columns:\n",
    "        metric = annotated_col.replace(\"annotated_\", \"\")\n",
    "        \n",
    "        # Initialize nested dictionaries for each metric\n",
    "        precision_results[metric] = {}\n",
    "        recall_results[metric] = {}\n",
    "        f1_results[metric] = {}\n",
    "        weighted_f1_results[metric] = {}\n",
    "        balanced_accuracy_results[metric] = {}\n",
    "        specificity_results[metric] = {}\n",
    "\n",
    "        # Find all prediction columns related to the current metric\n",
    "        relevant_prediction_cols = [col for col in prediction_columns if metric in col]\n",
    "        # Compare each model's predictions against the ground truth\n",
    "        for pred_col in relevant_prediction_cols:\n",
    "            model_name = pred_col.replace(f\"_{metric}_prediction\", \"\").replace(f\"{metric}_\", \"\")\n",
    "            ground_truth = dataframe[annotated_col]\n",
    "            predictions = dataframe[pred_col]\n",
    "\n",
    "            # Ensure ground truth and predictions are consistent data types\n",
    "            ground_truth = pd.Series(ground_truth).astype(int)\n",
    "            predictions = pd.Series(smart_convert_yes_no_list(predictions)).astype(int)\n",
    "\n",
    "            # Calculate evaluation scores for binary classification\n",
    "            f1 = f1_score(ground_truth, predictions)\n",
    "            f1_weighted = f1_score(ground_truth, predictions, average='weighted')\n",
    "            balanced_accuracy = balanced_accuracy_score(ground_truth, predictions)\n",
    "            precision = precision_score(ground_truth, predictions)\n",
    "            recall = recall_score(ground_truth, predictions)\n",
    "            \n",
    "            false_negatives = ((predictions == 0) & (ground_truth == 1)).sum()\n",
    "            true_negatives = ((predictions == 0) & (ground_truth == 0)).sum()\n",
    "\n",
    "            specificity = true_negatives / (true_negatives + false_negatives) if (false_negatives + true_negatives) > 0 else 0\n",
    "\n",
    "            # Store the results in the respective dictionaries\n",
    "            f1_results[metric][model_name] = str(round(f1 * 100, 1)) + '%'\n",
    "            weighted_f1_results[metric][model_name] = str(round(f1_weighted * 100, 1)) + '%'\n",
    "            balanced_accuracy_results[metric][model_name] = str(round(balanced_accuracy * 100, 1)) + '%'\n",
    "            precision_results[metric][model_name] = str(round(precision * 100, 1)) + '%'\n",
    "            recall_results[metric][model_name] = str(round(recall * 100, 1)) + '%'\n",
    "            specificity_results[metric][model_name] = str(round(specificity * 100, 1)) + '%'\n",
    "\n",
    "    # Convert the results nested dictionaries to DataFrames\n",
    "    precision_df = pd.DataFrame.from_dict(precision_results, orient='index')\n",
    "    recall_df = pd.DataFrame.from_dict(recall_results, orient='index')\n",
    "    f1_df = pd.DataFrame.from_dict(f1_results, orient='index')\n",
    "    weighted_f1_df = pd.DataFrame.from_dict(weighted_f1_results, orient='index')\n",
    "    balanced_accuracy_df = pd.DataFrame.from_dict(balanced_accuracy_results, orient='index')\n",
    "    specificity_df = pd.DataFrame.from_dict(specificity_results, orient='index')\n",
    "    return [['Weighted F1', weighted_f1_df],['Balanced Accuracy', balanced_accuracy_df], ['F1', f1_df], ['Precision', precision_df], ['Recall', recall_df], ['Specificity (True Negative Rate)', specificity_df]]#, ['False Discovery Rate', fdr_df], ['False Omission Rate', for_df]]\n",
    "\n",
    "def evaluate_batch(annotated_data, predictions_output=None, predict_only=False, end_index=1000, task_map=task_map):\n",
    "    annotated_data = annotated_data.progress_apply(lambda example: load_example_images(example), axis=1)\n",
    "    evaluation_data = enrich_with_task_details(annotated_data[:end_index], predictions_output, task_map=task_map)\n",
    "    evaluation_data['annotated_is_difference_captipn_accurate'] = evaluation_data['annotated_is_extensive_difference_caption_accurate'].copy()\n",
    "    if not predict_only:\n",
    "        return evaluate_models(evaluation_data), evaluation_data\n",
    "    return evaluation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hispanic-profession",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(ground_truth, predictions):\n",
    "    \"\"\"Calculates and returns a dictionary of evaluation metrics.\"\"\"\n",
    "    f1 = f1_score(ground_truth, predictions)\n",
    "    f1_weighted = f1_score(ground_truth, predictions, average='weighted')\n",
    "    balanced_accuracy = balanced_accuracy_score(ground_truth, predictions)\n",
    "    precision = precision_score(ground_truth, predictions)\n",
    "    recall = recall_score(ground_truth, predictions)\n",
    "\n",
    "    false_negatives = ((predictions == 0) & (ground_truth == 1)).sum()\n",
    "    true_negatives = ((predictions == 0) & (ground_truth == 0)).sum()\n",
    "\n",
    "    specificity = true_negatives / (true_negatives + false_negatives) if (false_negatives + true_negatives) > 0 else 0\n",
    "\n",
    "    return {\n",
    "        'f1': f1,\n",
    "        'f1_weighted': f1_weighted,\n",
    "        'balanced_accuracy': balanced_accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'specificity': specificity\n",
    "    }\n",
    "\n",
    "def store_metrics(results_dict, metrics, metric, model_name):\n",
    "    \"\"\"Stores calculated metrics into the results dictionaries.\"\"\"\n",
    "    results_dict['f1_results'][metric][model_name] = f\"{round(metrics['f1'] * 100, 1)}%\"\n",
    "    results_dict['weighted_f1_results'][metric][model_name] = f\"{round(metrics['f1_weighted'] * 100, 1)}%\"\n",
    "    results_dict['balanced_accuracy_results'][metric][model_name] = f\"{round(metrics['balanced_accuracy'] * 100, 1)}%\"\n",
    "    results_dict['precision_results'][metric][model_name] = f\"{round(metrics['precision'] * 100, 1)}%\"\n",
    "    results_dict['recall_results'][metric][model_name] = f\"{round(metrics['recall'] * 100, 1)}%\"\n",
    "    results_dict['specificity_results'][metric][model_name] = f\"{round(metrics['specificity'] * 100, 1)}%\"\n",
    "\n",
    "def evaluate_models(dataframe):\n",
    "    # Initialize dictionaries to store results for each metric\n",
    "    results_dict = {\n",
    "        'precision_results': {},\n",
    "        'recall_results': {},\n",
    "        'f1_results': {},\n",
    "        'weighted_f1_results': {},\n",
    "        'balanced_accuracy_results': {},\n",
    "        'specificity_results': {}\n",
    "    }\n",
    "\n",
    "    # Identify annotated columns and model predictions\n",
    "    annotated_columns = [col for col in dataframe.columns if col.startswith(\"annotated_\")]\n",
    "    prediction_columns = [col for col in dataframe.columns if \"_prediction\" in col]\n",
    "\n",
    "    # Iterate through each annotated column and calculate metrics\n",
    "    for annotated_col in annotated_columns:\n",
    "        metric = annotated_col.replace(\"annotated_\", \"\")\n",
    "\n",
    "        # Initialize nested dictionaries for each metric\n",
    "        for key in results_dict:\n",
    "            results_dict[key][metric] = {}\n",
    "\n",
    "        # Find all prediction columns related to the current metric\n",
    "        relevant_prediction_cols = [col for col in prediction_columns if metric in col]\n",
    "\n",
    "        # Oracle logic when both 'gpt-4o' and 'pipeline' exist and metric contains 'artifact'\n",
    "        if 'artifact' in metric and any('gpt-4o' in col for col in relevant_prediction_cols) and any('pipeline' in col for col in relevant_prediction_cols):\n",
    "            gpt4o_col = next(col for col in relevant_prediction_cols if 'gpt-4o' in col)\n",
    "            pipeline_col = next(col for col in relevant_prediction_cols if 'pipeline' in col)\n",
    "\n",
    "            # Get predictions for both models\n",
    "            gpt4o_predictions = pd.Series(smart_convert_yes_no_list(dataframe[gpt4o_col])).astype(int)\n",
    "            pipeline_predictions = pd.Series(smart_convert_yes_no_list(dataframe[pipeline_col])).astype(int)\n",
    "            ground_truth = pd.Series(dataframe[annotated_col]).astype(int)\n",
    "\n",
    "            # Create oracle prediction based on ground truth and model performance\n",
    "            oracle_predictions = pd.Series(0, index=ground_truth.index)\n",
    "            oracle_predictions[(ground_truth == 1) & ((gpt4o_predictions == 1) | (pipeline_predictions == 1))] = 1\n",
    "            oracle_predictions[(ground_truth == 0) & ((gpt4o_predictions == 0) | (pipeline_predictions == 0))] = 0\n",
    "\n",
    "            # Calculate metrics for the oracle model\n",
    "            oracle_metrics = calculate_metrics(ground_truth, oracle_predictions)\n",
    "            store_metrics(results_dict, oracle_metrics, metric, 'oracle - gpt-4o & pipeline')\n",
    "\n",
    "        # Compare each model's predictions against the ground truth\n",
    "        for pred_col in relevant_prediction_cols:\n",
    "            model_name = pred_col.replace(f\"_{metric}_prediction\", \"\").replace(f\"{metric}_\", \"\")\n",
    "            predictions = pd.Series(smart_convert_yes_no_list(dataframe[pred_col])).astype(int)\n",
    "            ground_truth = pd.Series(dataframe[annotated_col]).astype(int)\n",
    "\n",
    "            # Calculate metrics for the current model\n",
    "            model_metrics = calculate_metrics(ground_truth, predictions)\n",
    "            store_metrics(results_dict, model_metrics, metric, model_name)\n",
    "\n",
    "    # Convert results dictionaries to DataFrames\n",
    "    precision_df = pd.DataFrame.from_dict(results_dict['precision_results'], orient='index')\n",
    "    recall_df = pd.DataFrame.from_dict(results_dict['recall_results'], orient='index')\n",
    "    f1_df = pd.DataFrame.from_dict(results_dict['f1_results'], orient='index')\n",
    "    weighted_f1_df = pd.DataFrame.from_dict(results_dict['weighted_f1_results'], orient='index')\n",
    "    balanced_accuracy_df = pd.DataFrame.from_dict(results_dict['balanced_accuracy_results'], orient='index')\n",
    "    specificity_df = pd.DataFrame.from_dict(results_dict['specificity_results'], orient='index')\n",
    "\n",
    "    return [\n",
    "        ['Weighted F1', weighted_f1_df],\n",
    "        ['Balanced Accuracy', balanced_accuracy_df],\n",
    "        ['F1', f1_df],\n",
    "        ['Precision', precision_df],\n",
    "        ['Recall', recall_df],\n",
    "        ['Specificity (True Negative Rate)', specificity_df]\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef7fe8f-51b1-44b5-9ff6-a2145f735f09",
   "metadata": {},
   "source": [
    "#### Run Boolean Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe16636",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_evaluation_results(results_df_details, concat=False):\n",
    "    if concat:\n",
    "        combined_df = pd.DataFrame()\n",
    "        for metric_name, evaluation_df in results_df_details:\n",
    "            evaluation_df = evaluation_df.copy()  # Ensure we don't modify the original DataFrame\n",
    "            evaluation_df.index = [f\"{metric_name}_{idx.replace('_', ' ').title()}\" for idx in evaluation_df.index]\n",
    "            combined_df = pd.concat([combined_df, evaluation_df])\n",
    "        display(combined_df)\n",
    "    else:\n",
    "        for metric_name, evaluation_df in results_df_details:\n",
    "            display_title(metric_name)\n",
    "            evaluation_df.index = evaluation_df.index.str.replace('_', ' ').str.title()\n",
    "            display(evaluation_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "purple-husband",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def merge_new_columns_by_index(df_a, df_b):\n",
    "    new_columns_in_b = df_b.columns.difference(df_a.columns)\n",
    "    merged_df = pd.concat([df_a, df_b[new_columns_in_b]], axis=1)\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "junior-reasoning",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "LOCAL_RUN=False\n",
    "\n",
    "MODELS = ['gpt-4o', 'gpt-4o', 'gemini-1.5', 'gpt-4','gpt-4-turbo']#, 'qwen', 'intern-vl3'] # add gpt-4o-mini\n",
    "\n",
    "tasks_configuration = [\n",
    "      'is_artifacts',\n",
    "      'is_accurate',\n",
    "      'is_visualy_consistence',\n",
    "      'is_good_quality',\n",
    "      'is_extensive_difference_caption_accurate',\n",
    "      'is_difference_captipn_accurate'\n",
    "]\n",
    "\n",
    "annotated_data_majority = get_annotated_data_with_majority()\n",
    "for task_name in tasks_configuration:\n",
    "    display_title('--- ' + task_name.replace('_', ' ').title() + ' ---')\n",
    "    tasks_to_evaluate = {task_name: task_map.get(task_name)}\n",
    "    results_df, evaluation_data = evaluate_batch(annotated_data_majority, task_map=tasks_to_evaluate)\n",
    "    annotated_data_majority = merge_new_columns_by_index(annotated_data_majority, evaluation_data)    \n",
    "    display_evaluation_results([results_df[1]], concat=True)\n",
    "    display_evaluation_results(results_df, concat=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (llava)",
   "language": "python",
   "name": "llava"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
